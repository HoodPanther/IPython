{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scripts scrap data from the Utah Water Rights website, saves the data to text files, then parses the text files into a MySQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\pandas\\computation\\__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "from urllib2 import urlopen\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.ticker as tick\n",
    "import scipy.stats as sp\n",
    "import statsmodels.api as sm\n",
    "from pandas.stats.api import ols\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from pylab import rcParams\n",
    "import platform\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import urllib\n",
    "#import lxml\n",
    "import HTMLParser\n",
    "#from lxml import etree\n",
    "from cStringIO import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose output routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "route = 'C:\\\\Users\\\\Brooke\\\\Downloads'\n",
    "path = 'E:\\\\PROJECTS\\\\WR_DATA\\\\RawWellogs\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parser and Scraper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    # opens webpage for use in BeautifulSoup    \n",
    "    html = urlopen(url).read()\n",
    "    return BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Log Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapes well Logs from Water Rights website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Water Rights win number to begin search\n",
    "winbegin = 25000\n",
    "space = 1000\n",
    "winend = winbegin + space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while winbegin < 60000:\n",
    "       \n",
    "    # opens waterrights webpage by win   \n",
    "    for i in range(winbegin,winend):\n",
    "        try:\n",
    "            win = str(i)\n",
    "            soup = make_soup('http://waterrights.utah.gov/cgi-bin/docview.exe?Folder=welllog'+str(i))\n",
    "            souplist = soup.find('a', href=re.compile('^http://waterrights.utah.gov/docSys/v907/.*'))['href']\n",
    "            soupsite = make_soup(souplist)\n",
    "            souptext = soupsite.get_text()\n",
    "            g = path + 'log' + str(win).zfill(5) + '.txt'    \n",
    "            b = open(g, 'w')\n",
    "            b.write(souptext.encode('utf-8'))\n",
    "            b.close()\n",
    "        except TypeError:        \n",
    "            pass\n",
    "    \n",
    "    winbegin = winend\n",
    "    winend = winbegin + space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water System Use Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: http://www.waterrights.utah.gov/wateruse/WaterUseList.asp<br/>\n",
    "Example Pages of Input:<br/>\n",
    "http://www.waterrights.utah.gov/cgi-bin/wuseview.exe?Modinfo=Indview&SYSTEM_ID=11247<br/>\n",
    "http://www.waterrights.utah.gov/cgi-bin/wuseview.exe?Modinfo=Mgtview&SYSTEM_ID=11228<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def systemscraper(winbegin,winfinish,space,prefix,path):\n",
    "    '''\n",
    "    Systematically progresses though integer id numbers at the end of Water Rights URL to find system pages, \n",
    "    then saves those pages to text files.\n",
    "    \n",
    "    INPUT\n",
    "    -----\n",
    "    winbegin = integer value to start search\n",
    "    space = number of integers to search at a time\n",
    "    winfinish = integer value to end search\n",
    "    prefix = subset of systems to search (Modinfo= value in URL); can be Pws, Ind, or Mgt\n",
    "    path = place to store resulting text files\n",
    "    \n",
    "    OUTPUT\n",
    "    ------\n",
    "    text files in path labeled with corresponding integer values\n",
    "    '''\n",
    "    winend = winbegin + space\n",
    "\n",
    "    while winbegin < winfinish:\n",
    "\n",
    "        systemnm = []\n",
    "\n",
    "        # opens waterrights webpage by win   \n",
    "        for i in range(winbegin,winend):\n",
    "            try:\n",
    "                htmlplace = 'http://www.waterrights.utah.gov/cgi-bin/wuseview.exe?Modinfo=' + str(prefix) + 'view&SYSTEM_ID='+str(i)\n",
    "                soup = make_soup(htmlplace).get_text()\n",
    "                if \"ERROR: Use UNITS undefined\" in soup or len(soup) < 1000:\n",
    "                    pass\n",
    "                else:\n",
    "                    systemnm.append(str(i))\n",
    "                    g = path + str(prefix) + str(i).zfill(6) + '.txt'\n",
    "                    b = open(g, 'w')\n",
    "                    b.write(soup.encode('utf-8').strip())   \n",
    "                    b.close()\n",
    "            except TypeError:        \n",
    "                pass\n",
    "\n",
    "        winbegin = winend\n",
    "        winend = winbegin + space\n",
    "    print(\"Scanned %s to %s\"%(prefix,winfinish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'C:\\\\PROJECTS\\\\WR_DATA\\\\RawSystems\\\\'\n",
    "systems = ['Pws','Ind','Mgt']\n",
    "\n",
    "for i in systems:\n",
    "    systemscraper(1000,3000,1000,i,path)\n",
    "    systemscraper(10000,13000,1000,i,path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code searches through text captures of Water Rights html well files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = path + '*.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raises(exception_types, func, *args, **kw):\n",
    "    try:\n",
    "        func(*args, **kw)\n",
    "    except exception_types:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tparser(blurb):\n",
    "    '''\n",
    "    parses a snippet of text from gettext by removing and replaces extra spaces and return characters\n",
    "    '''\n",
    "    blurb = re.sub('\\r\\n      +', '\\n',str(blurb))\n",
    "    blurb = re.sub('\\r\\n +','\\r\\n',blurb)\n",
    "    blurb = re.sub(',',';',blurb)\n",
    "    blurb = re.sub(' +',',',blurb)\n",
    "    blurb = re.sub('\\r\\n','\\n',blurb)\n",
    "    blurb = re.sub('\\n\\n','\\n',blurb)\n",
    "    return blurb\n",
    "\n",
    "def gettext(strttext,endtext,snip):\n",
    "    '''\n",
    "    selects a subset of text by searching the text for a beginning string and an ending string\n",
    "    \n",
    "    INPUT\n",
    "    -----\n",
    "    strttext = string to find that begins text subset\n",
    "    endtext = string to find that ends the text subset\n",
    "    snip = text to subset\n",
    "    \n",
    "    OUTPUT\n",
    "    ------\n",
    "    b = subset of text; returns np.nan if no strttext is found\n",
    "    '''\n",
    "    \n",
    "    b = snip[snip.find(strttext)+len(strttext):snip.find(endtext,snip.find(strttext))].strip()\n",
    "    if snip.find(strttext) == -1:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Water Level Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = path +'*.txt'\n",
    "\n",
    "wl = []\n",
    "for f in glob.glob(filepath): \n",
    "    text = open(f).read()    \n",
    "\n",
    "    # grab section out of each text file for parsing\n",
    "    \n",
    "    rr =[]\n",
    "    wellcon = gettext(' WATER LEVEL DATA:','\\r\\n\\r\\n',text)\n",
    "    #print wellcon\n",
    "    if wellcon is not np.nan:\n",
    "        if len(wellcon) > 10:    \n",
    "            win = str(int(os.path.split(f)[1][3:8])).zfill(5)\n",
    "            rv = wellcon.split('\\n')\n",
    "            rr = []\n",
    "            for j in range(2,len(rv)):\n",
    "                if raises(ValueError, int, rv[j][0:21].strip(' ')[0:2])==False:\n",
    "                    rv[j] = win + ',' + rv[j][0:21] + ',' + rv[j][30:38].replace(',',';').strip(' ') + ',' + rv[j][38:].strip(' ')\n",
    "                \n",
    "                    rr.append(rv[j].replace('\\r','').replace('  ',' ').strip(' '))\n",
    "                else:\n",
    "                    pass\n",
    "            wl.append('\\n'.join(rr))\n",
    "\n",
    "levs = '\\n'.join(wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "waterlevels = pd.read_csv(StringIO(levs),names=['WIN','Date','Level','Method'],parse_dates=['Date'])\n",
    "waterlevels.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drilling Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = path +'*.txt'\n",
    "\n",
    "br = []\n",
    "for f in glob.glob(filepath): \n",
    "    text = open(f).read()    \n",
    "\n",
    "    # grab section out of each text file for parsing\n",
    "    \n",
    "    rr =[]\n",
    "    wellcon = gettext(' BOREHOLE INFORMATION:','\\r\\n\\r\\n',text)\n",
    "    #print wellcon\n",
    "    if wellcon is not np.nan:\n",
    "        if len(wellcon) > 10:    \n",
    "            win = str(int(os.path.split(f)[1][3:8])).zfill(5)\n",
    "            rv = wellcon.split('\\n')\n",
    "            rr = []\n",
    "            for j in range(2,len(rv)):\n",
    "                if raises(ValueError, int, rv[j][0:21].strip(' ')[0:2])==False:\n",
    "                    rv[j] = win + ',' + rv[j][0:17] + ',' + rv[j][17:23] + ',' + rv[j][23:29]+ ',' + rv[j][29:58]+ ',' + rv[j][58:]\n",
    "                \n",
    "                    rr.append(rv[j].replace('\\r','').replace('  ',' ').strip(' '))\n",
    "                    #print(rv[j])\n",
    "                else:\n",
    "                    pass\n",
    "            br.append('\\n'.join(rr))\n",
    "   \n",
    "bore = '\\n'.join(br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "borehole = pd.read_csv(StringIO(bore),names=['WIN','From (ft)','To (ft)','Diameter','Method','Fluid'])\n",
    "borehole.drop_duplicates(inplace=True)\n",
    "#borehole#.dropna(subset=['From (ft)','To (ft)'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drilling Activity Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = path +'*.txt'\n",
    "\n",
    "\n",
    "rr = []\n",
    "for f in glob.glob(filepath):\n",
    "    \n",
    "    text = open(f).read()    \n",
    "\n",
    "    # grab section out of each text file for parsing\n",
    "    \n",
    "    wellcon = gettext(' DRILLER ACTIVITIES:','\\r\\n\\r\\n',text)\n",
    "    \n",
    "    #print wellcon\n",
    "    if wellcon is not np.nan:\n",
    "        if len(wellcon) > 10:    \n",
    "            win = str(int(os.path.split(f)[1][3:8])).zfill(5)\n",
    "            \n",
    "            act1 = str(gettext('ACTIVITY # 1 ','\\r\\n\\r\\n',wellcon))\n",
    "            actnm1 = str(gettext('ACTIVITY # 1 ','\\r\\n',wellcon)).strip(' ')\n",
    "            drllr1 = str(gettext('DRILLER: ','LICENSE #:',act1)).replace(',',' ').strip(' ')\n",
    "            lic1 =  str(gettext('LICENSE #:','\\r\\n',act1)).strip(' ')\n",
    "            strt1 = str(gettext('START DATE: ','COMPLETION DATE: ',act1)).strip(' ')\n",
    "            comp1 = str(gettext('COMPLETION DATE: ','\\r\\n',act1)).strip(' ')\n",
    "            rr.append(win+','+actnm1+','+drllr1+','+lic1+','+strt1+','+comp1)\n",
    "            #print win+','+actnm1+','+drllr1+','+lic1+','+strt1+','+comp1\n",
    "            if 'ACTIVITY # 2' in wellcon: \n",
    "                act2 = str(gettext('ACTIVITY # 2 ','\\r\\n\\r\\n',wellcon))\n",
    "                actnm2 = str(gettext('ACTIVITY # 2 ','\\r\\n',wellcon)).strip(' ')\n",
    "                drllr2 = str(gettext('DRILLER: ','LICENSE #:',act2)).replace(',',' ').strip(' ')\n",
    "                lic2 =  str(gettext('LICENSE #:','\\r\\n',act2)).strip(' ')\n",
    "                strt2 = str(gettext('START DATE: ','COMPLETION DATE: ',act2)).strip(' ')\n",
    "                comp2 = str(gettext('COMPLETION DATE: ','\\r\\n',act2)).strip(' ')\n",
    "                rr.append(win+','+actnm2+','+drllr2+','+lic2+','+strt2+','+comp2)\n",
    "    \n",
    "            if 'ACTIVITY # 3' in wellcon:\n",
    "                act3 = str(gettext('ACTIVITY # 3 ','\\r\\n\\r\\n',wellcon))\n",
    "                actnm3 = str(gettext('ACTIVITY # 3 ','\\r\\n',wellcon)).strip(' ')\n",
    "                drllr3 = str(gettext('DRILLER: ','LICENSE #:',act3)).replace(',',' ').strip(' ')\n",
    "                lic3 =  str(gettext('LICENSE #:','\\r\\n',act3)).strip(' ')\n",
    "                strt3 = str(gettext('START DATE: ','COMPLETION DATE: ',act3)).strip(' ')\n",
    "                comp3 = str(gettext('COMPLETION DATE: ','\\r\\n',act3)).strip(' ')\n",
    "                rr.append(win+','+actnm3+','+drllr3+','+lic3+','+strt3+','+comp3)\n",
    "drill = '\\n'.join(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driller = pd.read_csv(StringIO(drill),names=['WIN','activity','driller','license','start','completion'],index_col=False)#,parse_dates=['start','completion'])\n",
    "driller.drop_duplicates(inplace=True)\n",
    "#lithlog.dropna(subset=['From (ft)','To (ft)'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lithology Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = path +'*.txt'\n",
    "\n",
    "lit = []\n",
    "for f in glob.glob(filepath): \n",
    "    text = open(f).read()    \n",
    "\n",
    "    # grab section out of each text file for parsing\n",
    "    \n",
    "    rr =[]\n",
    "    wellcon = gettext(' LITHOLOGY:','\\r\\n\\r\\n',text)\n",
    "    #print wellcon\n",
    "    if wellcon is not np.nan:\n",
    "        if len(wellcon) > 10:    \n",
    "            win = str(int(os.path.split(f)[1][3:8])).zfill(5)\n",
    "            rv = wellcon.split('\\n')\n",
    "            rr = []\n",
    "            for j in range(2,len(rv)):\n",
    "                if len(rv[j][0:7].strip(' ')) < 1: \n",
    "                    pass\n",
    "                else:\n",
    "                    rv[j] = win + ',' + rv[j][0:7] + ',' + rv[j][7:13].strip(' ') + ',' + rv[j][13:95].replace(',',';').strip(' ') + ',' + rv[j][95:108].strip(' ') +','+rv[j][108:]\n",
    "                    rr.append(rv[j].replace('\\r','').replace('  ',' ').strip(' '))\n",
    "            lit.append('\\n'.join(rr))\n",
    "\n",
    "                \n",
    "   \n",
    "lith = '\\n'.join(lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lithlog = pd.read_csv(StringIO(lith),names=['WIN','From (ft)','To (ft)','Material','Color','Comment'])\n",
    "lithlog.drop_duplicates(inplace=True)\n",
    "lithlog.dropna(subset=['From (ft)','To (ft)'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = path +'*.txt'\n",
    "\n",
    "const = []\n",
    "for f in glob.glob(filepath): \n",
    "    text = open(f).read()    \n",
    "\n",
    "    # grab section out of each text file for parsing\n",
    "\n",
    "    rev = []\n",
    "    win1 = []\n",
    "\n",
    "    wellcon = gettext('CASING:','\\r\\n\\r\\n',text)\n",
    "\n",
    "    if wellcon is not np.nan:\n",
    "        if len(wellcon) > 10:    \n",
    "            win = str(int(os.path.split(f)[1][3:8])).zfill(5)\n",
    "\n",
    "            rev = tparser(wellcon)\n",
    "            rv = rev.split('\\n')\n",
    "             \n",
    "            # column parsing\n",
    "            #print rv[i]\n",
    "            rr = win + ',,,,,'\n",
    "            for j in range(3,len(rv)):\n",
    "                t = rv[j].split(',')\n",
    "                t[0] = re.sub(' ',',',t[0])\n",
    "                t[0] = re.sub('\\+','-',t[0])\n",
    "                \n",
    "                rrr = []\n",
    "                if len(t) <= 2 or raises(ValueError, float, t[0]) == True:\n",
    "                    pass\n",
    "                elif len(t) == 3:\n",
    "                    firstpart = win + ',' + t[0] + ',' + t[1]\n",
    "                    if raises(ValueError, float, t[2]) == True:\n",
    "                        rr = firstpart + ',' + t[2] + ',,' \n",
    "                    elif t[2] < 1:\n",
    "                        rr = firstpart + ',,' + t[2] + ',' \n",
    "                    else:\n",
    "                        rr = firstpart + ',,,' + t[2] \n",
    "                elif len(t) == 4:\n",
    "                    firstpart = win + ',' + t[0] + ',' + t[1]\n",
    "                    if raises(ValueError, float, t[2]) == True and raises(ValueError, float, t[3]) == True:\n",
    "                        rr = firstpart + ',' + t[2] + ' ' + t[3] + ',,' \n",
    "                    elif raises(ValueError, float, t[2]) == True:\n",
    "                        if t[3] < 1:\n",
    "                            rr = firstpart + ',' + t[2] + ',' + t[3] + ',' \n",
    "                        else:\n",
    "                            rr = firstpart + ',' + t[2] + ',,' + t[3]\n",
    "                    else:\n",
    "                        rr = firstpart + ',,' + t[2] + ',' + t[3]\n",
    "                elif len(t)==5:\n",
    "                    firstpart = win + ',' + t[0] + ',' + t[1]\n",
    "                    if raises(ValueError, float, t[3]) == True:\n",
    "                        rr = firstpart + ',' + t[2] + ' ' + t[3] + ',,' + t[4]    \n",
    "                    elif t[3] > 5.0:\n",
    "                        rr = firstpart + ',' + t[2] + ',,' + t[4]\n",
    "                    else:\n",
    "                        rr = firstpart + ',' + t[2] + ',' + t[3] + ',' + t[4]\n",
    "                elif len(t) == 6 and raises(ValueError, float, t[4]) == True:\n",
    "                    if raises(ValueError, float, t[5]) == True:\n",
    "                        rr = firstpart + ',' + t[2] + ' ' + t[3] + ' ' + t[4] + ' ' + t[5] + ',,'                    \n",
    "                    else:\n",
    "                        rr = firstpart + ',' + t[2] + ' ' + t[3] + ' ' + t[4] + ',,' + t[5]\n",
    "                else:\n",
    "                    pass \n",
    "\n",
    "                const.append(rr)\n",
    "        else:\n",
    "            pass\n",
    "   \n",
    "constList = '\\n'.join(const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "construction = pd.read_csv(StringIO(constList),names=['WIN','From (ft)','To (ft)','Material','Gage (in)','Diameter (in)'])\n",
    "construction.drop_duplicates(inplace=True)\n",
    "construction.dropna(subset=['From (ft)','To (ft)'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screen Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = 'C:/PROJECTS/WR_DATA/RawWellogs/*.txt'\n",
    "scrnpath = 'C:/PROJECTS/WR_DATA/screen/'\n",
    "srn = []\n",
    "for f in glob.glob(filepath):    \n",
    "    text = open(f).read()   \n",
    "\n",
    "    scrntxt = gettext('SCREENS/PERFORATIONS:','\\r\\n\\r\\n',text)\n",
    "    \n",
    "    if scrntxt is not np.nan:\n",
    "        if len(scrntxt) > 10:    \n",
    "            rev = str(re.sub('\\r\\n      +', '\\n',scrntxt))\n",
    "            win = str(int(os.path.split(f)[1][3:8])).zfill(5)\n",
    "\n",
    "            rev = tparser(scrntxt)\n",
    "            rv = rev.split('\\n')\n",
    "    \n",
    "            for j in range(2, len(rv)):\n",
    "                if rv[j].count(',')==5:\n",
    "                    rv[j]= win + ',' + rv[j]\n",
    "                # add missing commas to end based on parsed length\n",
    "                elif rv[j].count(',') == 4:\n",
    "                    rvs = rv[j].split(',')\n",
    "                    if raises(ValueError, float, rvs[4]):\n",
    "                        rv[j] = win + ',' + ','.join(rvs[0:4]) + ',,' + rvs[4] \n",
    "                    else:\n",
    "                        rv[j] = win + ',' + ','.join(rvs[0:4]) + ',' + rvs[4] + ','\n",
    "                elif rv[j].count(',') < 4:\n",
    "                        rv[j] = win + ',' + rv[j] + ','*(5-rv[j].count(',')) \n",
    "\n",
    "            srn2 = '\\n'.join(rv[2:])\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        srn.append(srn2)\n",
    "scrrn = '\\n'.join(srn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "screendf = pd.read_csv(StringIO(scrrn),names=['WIN','From (ft)','To (ft)','Screen Type',\n",
    "                                    'Slot Size (in)','Screen Diameter (in)','Type or Num Perfs'])\n",
    "screendf.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pumping Test Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filepath = 'C:/PROJECTS/WR_DATA/RawWellogs/*.txt'\n",
    "pmpt = []\n",
    "for f in glob.glob(filepath):  \n",
    "    text = open(f).read()    \n",
    "   \n",
    "    win = str(int(os.path.split(f)[1][3:8])).zfill(5)\n",
    "\n",
    "    welltest = gettext('WELL TESTS:','\\r\\n\\r\\n\\r\\n ',text)\n",
    "    welltest = re.sub('/ /','/1/',str(welltest))\n",
    "    rev = tparser(welltest)\n",
    "    rv = rev.split('\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    if len(rev) > 10:           \n",
    "        \n",
    "        rv = rev.split('\\n')\n",
    "        #print(len(rv),rv[1:])\n",
    "\n",
    "        rr = []\n",
    "        for j in range(1,len(rv)):\n",
    "\n",
    "            t = rv[j].split(',')\n",
    "            if t[0] == '/' or t[0] == '/1/' or len(t[0]) < 8:\n",
    "                pass\n",
    "            elif len(t)==6:\n",
    "                if raises(ValueError, float, t[1]) == True and raises(ValueError, float, t[2]) == True:\n",
    "                    rr.append(win + ',' + t[0] + ',' + t[1]+ ' ' + t[2] + ',' + ','.join(t[3:]  ))\n",
    "                else:\n",
    "                    pass\n",
    "            elif len(t)==5:\n",
    "                if raises(ValueError, float, t[1]) == True and raises(ValueError, float, t[2]) == True and raises(ValueError, float, t[3]) == True:\n",
    "                    pass\n",
    "                elif raises(ValueError, float, t[1]) == True and raises(ValueError, float, t[2]) == True :\n",
    "                    pass\n",
    "                else:\n",
    "                    rr.append(win + ',' + rv[j])\n",
    "            elif len(t)==4: \n",
    "                if raises(ValueError, float, t[1]) == True and raises(ValueError, float, t[2]) == True and raises(ValueError, float, t[3]) == True:\n",
    "                    pass\n",
    "                elif raises(ValueError, float, t[1]) == True and raises(ValueError, float, t[2]) == True:\n",
    "                    pass\n",
    "                elif raises(ValueError, float, t[1]) == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    rr.append(win + ',,' + rv[j])\n",
    "            # add missing commas to end based on parsed length\n",
    "            else:\n",
    "                pass\n",
    "            pmp = '\\n'.join(rr)\n",
    "        pmpt.append(pmp)    \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "pump = '\\n'.join(pmpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pumpingtests = pd.read_csv(StringIO(pump), names=['WIN', 'Date', 'Method', 'Yield (cfs)', \n",
    "                                                  'Drawdown (ft)', 'Pump Duration (hr)'],parse_dates=['Date'])\n",
    "pumpingtests.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    g=[]\n",
    "    \n",
    "    for i in range(len(rev)):\n",
    "        g.append(testpath + 'test' + str(win1[i]).zfill(5) + '.csv')    \n",
    "        b = open(g[i], 'w')\n",
    "        b.write(rev[i])\n",
    "\n",
    "\n",
    "\n",
    "fout=open(testpath+\"out.csv\",\"a\")\n",
    "# first file:\n",
    "\n",
    "testcombpath = testpath+\"*.csv\"\n",
    "for line in open(testpath + \"test00001.csv\"):\n",
    "    fout.write(line)\n",
    "# now the rest:    \n",
    "for testfile in glob.glob(testcombpath):\n",
    "    f = open(testfile)\n",
    "    lines = f.readlines()[3:]    \n",
    "    fout.write(\"\\n\")    \n",
    "    for g in lines:\n",
    "        fout.write(g)    \n",
    "    f.close() # not really needed\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System and Source Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pathname = 'C:\\\\PROJECTS\\\\WR_DATA\\\\RawSystems\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files = glob.glob(pathname +'*.txt')\n",
    "\n",
    "sourcet, conn, use = {}, {}, {}\n",
    "\n",
    "indcode, systype, sysnum, link, sysname, city, county, syscat, huc, pwsid, deqcat, numberofsources = [],[],[],[],[],[],[],[],[],[],[],[]\n",
    "source, system, systemid, pls, sourcetype, sourceuse, win, wrnum, sourcecode,sourceid = [],[],[],[],[],[],[],[],[],[]\n",
    "system_id = []\n",
    "\n",
    "for f in range(len(files)): \n",
    "    text = open(files[f]).read()\n",
    "    systemname = gettext('System  Name:','Address:',text)\n",
    "    sid = gettext('Public Water System ID:','DEQ',text)\n",
    "    srcind = [m.start() for m in re.finditer('Source Summary', text)]\n",
    "    \n",
    "    prefix = os.path.split(files[f])[1][0:3]\n",
    "    html = 'http://www.waterrights.utah.gov/cgi-bin/wuseview.exe?Modinfo='+ prefix +'view&SYSTEM_ID='\n",
    "    \n",
    "    sysnum.append(int(os.path.split(files[f])[1][3:9]))\n",
    "    linknum = int(os.path.split(files[f])[1][3:9])\n",
    "    systype.append(prefix)\n",
    "    link.append(html+str(linknum))\n",
    "    \n",
    "    systid = prefix+'-'+str(linknum).zfill(5)\n",
    "    \n",
    "    system_id.append(systid)\n",
    "    sysname.append(gettext('System  Name:','Address:',text))\n",
    "    city.append(gettext('City:','State:',text))\n",
    "    county.append(gettext('County:','Primary Use:',text))\n",
    "    syscat.append(gettext('Primary Use:','Standard',text))\n",
    "    huc.append(gettext('Hydro Unit Code:','Public',text))\n",
    "    pwsid.append(gettext('Public Water System ID:','DEQ',text))\n",
    "    deqcat.append(gettext('DEQ System Category:','\\n',text))\n",
    "    indcode.append(gettext('Standard Industrial Code:','Dual',text))\n",
    "    \n",
    "    numberofsources.append(len(srcind))\n",
    "    \n",
    "    for i in range(len(srcind)):\n",
    "\n",
    "        if i == len(srcind)-1:\n",
    "            subtext = text[srcind[i]:-1]\n",
    "        else:\n",
    "            subtext = text[srcind[i]:srcind[i+1]]\n",
    "            \n",
    "        source.append(gettext('Source Name:','\\n',subtext))\n",
    "        pls.append(gettext('PLS Location:','\\n',subtext))\n",
    "        sourcetype.append(gettext('Source Type:','\\n',subtext))\n",
    "        sourceuse.append(gettext('Primary Use:','\\n',subtext))\n",
    "        win.append(gettext('Well ID Number:','(C',subtext))\n",
    "        sourcecode.append(gettext('DEHN Source Code:','\\n',subtext))\n",
    "        wrnum.append(gettext('Water Right Numbers:','\\n',subtext))\n",
    "        system.append(systemname)\n",
    "        systemid.append(sid)\n",
    "        srcid = systid +'-'+str(i).zfill(2)\n",
    "        sourceid.append(srcid)\n",
    "        \n",
    "        table = gettext(' Source Record (ACFT)\\r\\n','\\r\\n \\r',subtext)\n",
    "        table = re.sub('Master +Meter','MasterMeter',str(table))\n",
    "        table = re.sub('Master +Met','MasterMeter',table)\n",
    "        table = re.sub('Individual +Meters','IndividualMeters',table)\n",
    "        table = re.sub('Measuring +Method','MeasuringMethod',table)\n",
    "        table = tparser(table) \n",
    "        rv = table.split('\\n')\n",
    "        b = []\n",
    "        for j in range(len(rv)):    \n",
    "            if rv[j].count(',') > 15:\n",
    "                rb = rv[j].split(',')\n",
    "                rb.insert(15,'\\n')\n",
    "                b.append(','.join(rb))\n",
    "            elif rv[j].count(',')==15:\n",
    "                b.append(rv[j])  \n",
    "            elif rv[j].count(',') < 15 and rv[j].count(',') > 4:\n",
    "                b.append(rv[j] + ','*(15-rv[j].count(',')))\n",
    "            else:\n",
    "                pass\n",
    "        rev = '\\n'.join(b)\n",
    "        try:\n",
    "            sourcet[srcid] = pd.read_csv(StringIO(rev))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    usetable = gettext(' Annual Use Info (Acft) \\r\\n','\\r\\n \\r',text)\n",
    "    usetable = tparser(usetable)\n",
    "    try:\n",
    "        use[systid] = pd.read_csv(StringIO(usetable))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    conntable = gettext(' Annual Connection Info\\r\\n','\\r\\n\\r\\n ',text)\n",
    "    conntable = tparser(conntable)\n",
    "    try:\n",
    "        conn[systid] = pd.read_csv(StringIO(conntable))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sysdict = {'systype':systype, 'systemnum': sysnum, 'link':link, 'sysname':sysname, 'city':city, \n",
    "           'county':county, 'syscat':syscat, 'indust code':indcode, 'number of sources':numberofsources,\n",
    "          'huc':huc, 'pwsid':pwsid,'deqcat':deqcat, 'systemid':system_id}\n",
    "\n",
    "systems = pd.DataFrame(sysdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sourcedict = {'source':source, 'system id': systemid, 'system':system, 'pls':pls, 'source type': sourcetype,\n",
    "          'source use':sourceuse, 'win':win, 'wrnum':wrnum, 'DEHN source id':sourcecode, 'source id':sourceid}\n",
    "\n",
    "sources = pd.DataFrame(sourcedict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sourcetake = pd.concat(sourcet)\n",
    "sourcetake.reset_index(inplace=True)\n",
    "sourcetake.rename(columns={'level_0':'systemid'},inplace=True)\n",
    "sourcetake.set_index(['systemid','Year'],inplace=True)\n",
    "sourcetake.drop(['level_1','Measuring','MeasuringMethod','Unnamed: 15','Mea','Meth','Ann'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "srctake = sourcetake.stack().to_frame()\n",
    "srctake.rename(columns={'0':'Use (ac-ft)'},inplace=True)\n",
    "srctake.reset_index(inplace=True)\n",
    "srctake['Year'] = pd.to_numeric(srctake['Year'],errors='coerce')\n",
    "srctake = srctake[(srctake['Year']<=datetime.today().year)&(srctake['Year']>=1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srctake = srctake[srctake['level_2'].isin(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])]\n",
    "srctake['my'] = srctake[['Year','level_2']].apply(lambda x: pd.to_datetime(str(int(x[0]))+' '+str(x[1]), format='%Y %b'),1)\n",
    "srctake.columns = ['sourceid','Year','Month','Use','datetime']\n",
    "srctake['systemid'] = srctake['sourceid'].apply(lambda x: str(x)[0:9],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "systemuse = pd.concat(use)\n",
    "systemuse.reset_index(inplace=True)\n",
    "systemuse.rename(columns={'level_0':'systemid'},inplace=True)\n",
    "systemuse.drop(['level_1','Tota','nan'],axis=1,inplace=True)\n",
    "cols = ['Commercial','Domestic','Industrial','Institutnl','Other','Stock','Unmetered','Wholesale']\n",
    "for col in cols:\n",
    "    systemuse[col] = pd.to_numeric(systemuse[col], errors='coerce')\n",
    "systemuse['Total'] = pd.to_numeric(systemuse['Total'])\n",
    "systemuse['Total1'] = systemuse[cols].sum(axis=1)\n",
    "systemuse['Year'] = pd.to_numeric(systemuse['Year'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "systemuseData = systemuseData[systemuseData['Year']<2017]\n",
    "systemuseData = systemuseData[systemuseData.Total < 100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connections = pd.concat(conn)\n",
    "connections.reset_index(inplace=True)\n",
    "connections.rename(columns={'level_0':'systemid'},inplace=True)\n",
    "connections.drop(['level_1','nan'],axis=1,inplace=True)\n",
    "connections = connections[(connections['Year']<datetime.datetime.today().year)&(connections['Year']>1830)]\n",
    "connections = connections[connections.Total < 100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This must be done after scraping or you will throw a host error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(route)\n",
    "import enginegetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine = enginegetter.getEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systems and Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the systems made up of the sources.  They are often cities or water agencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "systems.to_sql(con=engine, name='systems', if_exists='replace', flavor='mysql',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the water use sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources.to_sql(con=engine, name='sources', if_exists='replace', flavor='mysql', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depicts the amount of water use by each source in ac-ft/mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srctake.to_sql(con=engine, name='sourceuse', if_exists='replace', flavor='mysql',chunksize=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depicts the amount of water use by each system in ac-ft/yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "systemuseData.to_sql(con=engine, name='systemuse', if_exists='replace', flavor='mysql',chunksize=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depicts number of connections in system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connections.to_sql(con=engine, name='systemconnections', if_exists='replace', flavor='mysql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "screendf.to_sql(con=engine, name='wellscreens', if_exists='replace', flavor='mysql',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "construction.to_sql(con=engine, name='construction', if_exists='replace', flavor='mysql',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pumpingtests.to_sql(con=engine, name='pumpingtests', if_exists='replace', flavor='mysql',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "waterlevels.to_sql(con=engine, name='waterlevels', if_exists='replace', flavor='mysql',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "borehole.to_sql(con=engine, name='borehole', if_exists='replace', flavor='mysql',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driller.to_sql(con=engine, name='driller', if_exists='replace', flavor='mysql',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lithlog.to_sql(con=engine, name='lithlog', if_exists='replace', flavor='mysql',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ArcPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Locate ArcPy and add it to the path\n",
    "Created on 13 Feb 2015\n",
    "@author: Jamesramm\n",
    "https://github.com/JamesRamm/archook/blob/master/archook.py\n",
    "'''\n",
    "import _winreg\n",
    "import sys\n",
    "from os import path\n",
    "def locate_arcgis():\n",
    "  '''\n",
    "  Find the path to the ArcGIS Desktop installation.\n",
    "  Keys to check:\n",
    "  HLKM/SOFTWARE/ESRI/ArcGIS 'RealVersion' - will give the version, then we can use\n",
    "  that to go to\n",
    "  HKLM/SOFTWARE/ESRI/DesktopXX.X 'InstallDir'. Where XX.X is the version\n",
    "  We may need to check HKLM/SOFTWARE/Wow6432Node/ESRI instead\n",
    "  '''\n",
    "  try:\n",
    "    key = _winreg.OpenKey(_winreg.HKEY_LOCAL_MACHINE,\n",
    "                          'SOFTWARE\\\\Wow6432Node\\\\ESRI\\\\ArcGIS', 0)\n",
    "\n",
    "    version = _winreg.QueryValueEx(key, \"RealVersion\")[0][:4]\n",
    "\n",
    "    key_string = \"SOFTWARE\\\\Wow6432Node\\\\ESRI\\\\Desktop{0}\".format(version)\n",
    "    desktop_key = _winreg.OpenKey(_winreg.HKEY_LOCAL_MACHINE,\n",
    "                                  key_string, 0)\n",
    "\n",
    "    install_dir = _winreg.QueryValueEx(desktop_key, \"InstallDir\")[0]\n",
    "    return install_dir\n",
    "  except WindowsError:\n",
    "    raise ImportError(\"Could not locate the ArcGIS directory on this machine\")\n",
    "\n",
    "def get_arcpy():  \n",
    "  '''\n",
    "  Allows arcpy to imported on 'unmanaged' python installations (i.e. python installations\n",
    "  arcgis is not aware of).\n",
    "  Gets the location of arcpy and related libs and adds it to sys.path\n",
    "  '''\n",
    "  install_dir = locate_arcgis()  \n",
    "  arcpy = path.join(install_dir, \"arcpy\")\n",
    "  # Check we have the arcpy directory.\n",
    "  if not path.exists(arcpy):\n",
    "    raise ImportError(\"Could not find arcpy directory in {0}\".format(install_dir))\n",
    "\n",
    "  # First check if we have a bin64 directory - this exists when arcgis is 64bit\n",
    "  bin_dir = path.join(install_dir, \"bin64\")\n",
    "  if not path.exists(bin_dir):\n",
    "    # Fall back to regular 'bin' dir otherwise.\n",
    "    bin_dir = path.join(install_dir, \"bin\")\n",
    "\n",
    "  scripts = path.join(install_dir, \"ArcToolbox\", \"Scripts\")  \n",
    "  sys.path.extend([arcpy, bin_dir, scripts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_arcpy()\n",
    "import arcpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileloc = r'C:/GIS/WR_DATA.gdb/'\n",
    "\n",
    "def df2gdb(df,fileloc,name):\n",
    "    x = np.array(np.rec.fromrecords(df.values))\n",
    "    names = df.dtypes.index.tolist()\n",
    "    x.dtype.names = tuple(names)\n",
    "    arcpy.da.NumPyArrayToTable(x, fileloc+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2gdb(systems,fileloc,'systems')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2gdb(sources,fileloc,'sources')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2gdb(systemuseData,fileloc,'systemuse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2gdb(screendf,fileloc,'wellscreens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2gdb(connections,fileloc,'connections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df2gdb(construction,fileloc,'construction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2gdb(pumpingtests,fileloc,'pumpingtests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot and Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "systemuseData.groupby('Year')[['Total','Domestic','Industrial','Commercial']].sum().plot()\n",
    "plt.xlim(1980,2020)\n",
    "plt.ylabel('Use (ac-ft)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "systemuseData.groupby('Year')['Total'].sum().plot()\n",
    "plt.xlim(1980,2020)\n",
    "plt.ylabel('Use (ac-ft)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('C:\\\\PROJECTS\\\\WR_DATA\\\\' + 'systems_and_sources.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "systems.to_excel(writer, sheet_name='systems')\n",
    "sources.to_excel(writer, sheet_name='sources')\n",
    "srctakeData.to_excel(writer, sheet_name='sourcetake')\n",
    "systemuseData.to_excel(writer, sheet_name='system_use')\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
