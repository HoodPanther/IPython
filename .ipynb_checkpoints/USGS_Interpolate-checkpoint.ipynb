{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the Scientific Python (scipy) stack tools to generate flow duration curves from current USGS NWIS data.\n",
    "\n",
    "Using recipes from this notebook, you can make:\n",
    "* USGS Station Summaries\n",
    "* Flow duration curves\n",
    "* Iterative import and compilation of USGS station information and data\n",
    "* boxplots using pandas\n",
    "* iterative charts (one monthly summary boxplot per station)\n",
    "* Gantt charts of USGS stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this for some great `pandas` applications:\n",
    "http://earthpy.org/time_series_analysis_with_pandas_part_2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "from pyproj import Proj, transform\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "from datetime import datetime, date, timedelta\n",
    "import mechanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "huc = '16010203'\n",
    "stationhtml = \"http://waterservices.usgs.gov/nwis/site/?format=rdb,1.0&huc=\"+str(huc)+\"&siteType=GW&hasDataTypeCd=gw\"\n",
    "response = urllib2.urlopen(stationhtml)\n",
    "html = response.read()\n",
    "skip = html[:html.rfind('#\\n')+2].count('\\n')\n",
    "skiplist = range(0,skip)\n",
    "skiplist.append(skip+1)\n",
    "sites = pd.read_table(stationhtml,sep=\"\\t\",skiprows=skiplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations = list(sites['site_no'].values)\n",
    "stations = [str(i) for i in stations]\n",
    "stations = ', '.join(stations)\n",
    "stations = stations.replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stationinfohtml = \"http://waterservices.usgs.gov/nwis/site/?format=rdb&sites=\"+stations+\"&siteOutput=expanded\"\n",
    "response = urllib2.urlopen(stationinfohtml)\n",
    "html = response.read()\n",
    "skip = html[:html.rfind('#\\n')+2].count('\\n')\n",
    "skiplist = range(0,skip)\n",
    "skiplist.append(skip+1)\n",
    "siteinfo = pd.read_table(stationinfohtml, sep=\"\\t\",skiprows=skiplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'agency_cd', u'site_no', u'station_nm', u'site_tp_cd', u'lat_va', u'long_va', u'dec_lat_va', u'dec_long_va', u'coord_meth_cd', u'coord_acy_cd', u'coord_datum_cd', u'dec_coord_datum_cd', u'district_cd', u'state_cd', u'county_cd', u'country_cd', u'land_net_ds', u'map_nm', u'map_scale_fc', u'alt_va', u'alt_meth_cd', u'alt_acy_va', u'alt_datum_cd', u'huc_cd', u'basin_cd', u'topo_cd', u'instruments_cd', u'construction_dt', u'inventory_dt', u'drain_area_va', u'contrib_drain_area_va', u'tz_cd', u'local_time_fg', u'reliability_cd', u'gw_file_cd', u'nat_aqfr_cd', u'aqfr_cd', u'aqfr_type_cd', u'well_depth_va', u'hole_depth_va', u'depth_src_cd', u'project_no'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siteinfo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getelev(x):\n",
    "    elev = \"http://ned.usgs.gov/epqs/pqs.php?x=\"+str(x[0])+\"&y=\"+str(x[1])+\"&units=Meters&output=xml\"\n",
    "    response = urllib2.urlopen(elev)\n",
    "    html = response.read()\n",
    "    d = xmltodict.parse(html)\n",
    "    return float(d['USGS_Elevation_Point_Query_Service']['Elevation_Query']['Elevation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getwlelev(x):\n",
    "    return x[1] - (x[0]/3.2808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qqq(x):\n",
    "    x.rstrip().lstrip()\n",
    "    j = x.split(' ')\n",
    "    a = j[0][:1]\n",
    "    b = j[0][1:]\n",
    "    c = j[1][:1]\n",
    "    d = j[1][1:]\n",
    "    e = [a,b,c,d,j[2],j[3],j[4],j[5],j[6]]\n",
    "    \n",
    "    NS = int(e[1].replace(',',''))\n",
    "    EW = int(e[3].replace(',',''))\n",
    "    qc = e[4]\n",
    "    d1 = e[0]\n",
    "    d2 = e[2]\n",
    "    dic1 = {'NE':'a','NW':'b','SW':'c','SE':'d'}\n",
    "    qcdDict = {'E4S':'d','E4N':'a','N4E':'a','N4W':'b','W4N':'b','W4S':'c','S4W':'c','S4E':'d'}\n",
    "    dic2 = {'a':'b','b':'a','c':'d','d':'c'}\n",
    "    dic3 = {'a':'d','b':'c','c':'b','d':'a'}\n",
    "    dic4 = {'a':'c','b':'d','c':'a','d':'b'}\n",
    "    if qc[-1]=='4':\n",
    "        if qc[0]=='N' or qc[0]=='S':\n",
    "            qcd = qc+d2\n",
    "        elif qc[0]=='E' or qc[0]=='W':\n",
    "            qcd = qc+d1\n",
    "        q1 = qcdDict.get(qcd,'x')\n",
    "    elif qc in ('NE','NW','SW','SE'):\n",
    "        q1 = dic1.get(qc)\n",
    "    else:\n",
    "        print \"invalid quarter\"\n",
    "        q1 = 'X'\n",
    "    if NS < 1320:\n",
    "        if EW <1320:\n",
    "            q2 = q1\n",
    "        elif EW >1320:\n",
    "            qd2 = {'a':'b','b':'a','c':'d','d':'c'}\n",
    "            q2 = dic2.get(q1,'x')\n",
    "    elif NS > 1320:\n",
    "        if EW <1320:\n",
    "            q2 = dic3.get(q1,'x')\n",
    "        elif EW >1320:\n",
    "            q2 = dic4.get(q1,'x')\n",
    "    else:\n",
    "        q2 = 'X'\n",
    "\n",
    "    if NS < 660 or (NS > 1320 and NS < 1980):\n",
    "        if (EW < 660) or (EW > 1320 and EW < 1980):\n",
    "            q3 = q1\n",
    "        elif (EW > 660 and EW < 1320) or (EW > 1980 and EW < 2640):\n",
    "            q3 = dic2.get(q1,'x')\n",
    "    elif (NS > 660 and NS < 1320) or (NS > 1980 and NS < 2640):\n",
    "        if (EW < 660) or (EW > 1320 and EW < 1980):\n",
    "            q3 = dic3.get(q1,'x')\n",
    "        elif (EW > 660 and EW < 1320) or (EW > 1980 and EW < 2640):\n",
    "            q3 = dic4.get(q1,'x')\n",
    "    else:\n",
    "        q3 = 'X'\n",
    "    Tn = e[6][:-1].rjust(2)\n",
    "    Rn = e[7][:-1].rjust(2)\n",
    "    Sec = e[5].rjust(2)\n",
    "    TRd = e[6][-1]+e[7][-1]\n",
    "    TR = dic1.get(TRd).upper()\n",
    "    CAD = '('+TR+'-'+Tn+'-'+Rn+')'+Sec+q1+q2+q3+'-1'\n",
    "    return CAD                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getPLSS(desc):\n",
    "    PLSStitlen = len(\"PLSS Description is </font><font size='4'> \")\n",
    "    PLSSbeg = desc.find(\"PLSS Description is </font><font size='4'>\")+PLSStitlen\n",
    "    PLSSend = desc[PLSSbeg:].find(\" <br> \")+PLSSbeg\n",
    "    PLSSdesc = desc[PLSSbeg:PLSSend]\n",
    "    QRTRend = desc.find(\" of the above Section </font></center><p>\")\n",
    "    QRTRbeg = desc.find(\"The point is found in the \")+len(\"The point is found in the \")\n",
    "    QRTRdesc = desc[QRTRbeg:QRTRend]\n",
    "    SecBeg = PLSSdesc.find(\"Section \")+len(\"Section \")\n",
    "    SecEnd = PLSSdesc.find(\", Township \")\n",
    "    Section = int(PLSSdesc[SecBeg:SecEnd])\n",
    "    TownBeg = SecEnd + len(\", Township \")\n",
    "    TownEnd = PLSSdesc.find(\", Range \")\n",
    "    Township = PLSSdesc[TownBeg:TownEnd]\n",
    "    RangeBeg = TownEnd + len(\", Range \")\n",
    "    RangeEnd = RangeBeg + PLSSdesc[RangeBeg:].find(\", \")\n",
    "    Range = PLSSdesc[RangeBeg:RangeEnd]\n",
    "\n",
    "    PLSS = PLSSdesc.replace(\"South \",\"S\")\n",
    "    PLSS = PLSS.replace(\"West \",\"W\")\n",
    "    PLSS = PLSS.replace(\"North \",\"N\")\n",
    "    PLSS = PLSS.replace(\"East \",\"E\")\n",
    "    PLSS = PLSS.replace(\"feet \",\"\")\n",
    "    PLSS = PLSS.replace(\" and \",\" \")\n",
    "    PLSS = PLSS.replace(\"from the \",\"\")\n",
    "    BM = PLSS[-6:-4]\n",
    "    PLSScn = PLSS.find(\" Corner\")\n",
    "    PLSS = PLSS[:PLSScn]\n",
    "    PLSS = PLSS.replace(',','')\n",
    "    PLSS = PLSS + \" \" +str(Section) + \" \" + Township + \" \" + Range + \" \" + BM\n",
    "    \n",
    "    try:\n",
    "        CAD = qqq(PLSS)\n",
    "    except(AttributeError,TypeError,UnboundLocalError):\n",
    "        print PLSS\n",
    "        CAD = np.nan\n",
    "    return PLSS, CAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A- 9- 1) 3bba-1\n"
     ]
    }
   ],
   "source": [
    "PLSS = 'N337 E1,117 W4 3 9N 1E SL'\n",
    "print qqq(PLSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proj(x):\n",
    "    inProj = Proj(init='epsg:4326') #WGS84\n",
    "    outProj = Proj(init='epsg:2152') #NAD83(CSRS98) / UTM zone 12N\n",
    "    x2,y2 = transform(inProj,outProj,x[0],x[1])\n",
    "    return x2, y2\n",
    "\n",
    "siteinfo['UTM_X'], siteinfo['UTM_Y'] = zip(*siteinfo[['dec_long_va','dec_lat_va']].apply(lambda x: proj(x),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def USGSID(x):\n",
    "    def dms(dec):\n",
    "        DD = str(int(abs(dec)))\n",
    "        MM = str(int((abs(dec) - int(DD))*60)).zfill(2)\n",
    "        SS = str(int(round((((abs(dec) - int(DD))*60) - int(MM))*60, 0))).zfill(2)\n",
    "        return DD+MM+SS\n",
    "    return 'UT'+dms(x[1])+dms(x[0])+'01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getwellinfo(x):\n",
    "    request = mechanize.Request(\"http://maps.waterrights.utah.gov/asp/location.asp\")\n",
    "    response = mechanize.urlopen(request)\n",
    "    forms = mechanize.ParseResponse(response, backwards_compat=False)\n",
    "    response.close()\n",
    "    form = forms[0]\n",
    "    form[\"UTMx\"]= str(x[0])\n",
    "    form[\"UTMy\"]= str(x[1])\n",
    "    form[\"datumutm\"]=[\"NAD83\"]\n",
    "    desc =  mechanize.urlopen(form.click()).read()\n",
    "    try:\n",
    "        PLSS, CAD = getPLSS(desc)\n",
    "    except(ValueError):\n",
    "        PLSS, CAD = np.nan, np.nan\n",
    "    return PLSS, CAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(429195.65389178065, 4603579.3509103218, 19670905.0, 155.0)\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "x = siteinfo.ix[i].UTM_X, siteinfo.ix[i].UTM_Y, siteinfo.ix[i].construction_dt, siteinfo.ix[i].well_depth_va\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1967\n",
      "155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\">\\r\\n\\r\\n<script>\\r\\n\\t      var loggedIn=\\'false\\'\\r\\n        var loggedInName=\\'\\'\\r\\n</script>\\r\\n<script>\\r\\n  var handheld = 0\\r\\n  var notIE = 0\\r\\n  var userAgentValue = 0\\r\\n</script>\\r\\n\\r\\n<script>userAgentValue = \"Python-urllib/2.7\"</script>\\r\\n\\r\\n\\r\\n<head>\\r\\n<title>Well Log Search - Utah Water Rights</title>\\r\\n\\r\\n</head>\\r\\n<body onload=\"startup()\"> \\r\\n<script language=\"javascript\" src=\"http://waterrights.utah.gov/header.js\" type=\"text/javascript\"></script>\\r\\n<map name=\"logomap\">\\r\\n<area shape=\"rect\" alt=\"Utah Division of Water Rights\" coords=\"0,0,755,40\" href=\"/default.asp\">\\r\\n<area shape=\"default\" nohref>\\r\\n</map>\\r\\n<img SRC=\"../images/logobar.gif\" BORDER=0 usemap=\"#logomap\" >\\t\\r\\n<table border=\"0\" width=\"955\" cellspacing=\"0\" cellpadding=\"0\">\\r\\n<tr>\\r\\n<td>&nbsp;&nbsp;&nbsp;</td>\\r\\n<td>\\r\\n \\r\\n<br>\\r\\n<h2><b>Well Log Search </b></h2>\\r\\n\\r\\n\\r\\n  \\r\\n\\r\\n<h3> Well Logs within 1000 ft of X: 429195.653892 Y: 4603579.35091</h3><input type=\\'BUTTON\\' name=\\'Key\\' value=\\'View Map\\' onclick=\\'viewGoogleMap();\\'> <input type=\\'BUTTON\\' name=\\'Key\\' value=\\'Quit\\' onclick=\\'history.go(-1);\\')><br><br>Listed below is information on wells which are tied to water rights or non-production well permits.  Wells older than 1992 may not be connected to the permit database. <p> \\r\\n    \\r\\n    \\t\\r\\n    <table border=0 cellpadding=0>\\r\\n    <tr>\\r\\n    <b><td style=\"width:150px;\">WRNUM/Appl. No.</td>\\r\\n\\r\\n       <td style=\"width:100px;\">Distance From Point (ft) </td>\\r\\n\\r\\n       <td style=\"width:100px;\">Diameter</td>\\r\\n       <td style=\"width:100px;\">Depth</td>\\r\\n       <td style=\"width:100px;\">Drilled Date</td>\\r\\n       <td style=\"width:350px;\">Location(link to Log)</td>\\r\\n       <td style=\"width:150px;\">WIN</td></b>\\r\\n       <td style=\"width:150px;\">Geologic Log</td></b>\\r\\n    </tr>\\r\\n    \\r\\n<tr><td><a href=\\'http://waterrights.utah.gov/cgi-bin/wrprint.exe?wrnum=25-10783\\' target=\\'_blank\\'>25-10783</a></td><td>882</td><td>155.0</td><td>155.0</td><td>08/25/1967</td><td><a href=\\'http://www.waterrights.utah.gov/wellinfo/welldrilling/wlbrowse.asp?WIN=432043\\' target=\\'_blank\\'>S 975 W 4572 NE 28  10N 1E SL</a></td><td>432043</td><td> </td></tr>\\r\\n     </table>\\r\\n     \\r\\n     \\r\\n     <!-- Set up a form to send this data to the mapserver -->\\r\\n     <form name=\"mapform\" id=\"mapform\" method=\"POST\" action=\"http://maps.waterrights.utah.gov/cgi-bin/mapserv.exe\" target=\\'_blank\\'>\\r\\n\\r\\n       <input type=\\'hidden\\' name=\\'map\\' value=\\'d:/wrtdata/public/mapserver/welllog/welllog.map\\'>\\r\\n       \\r\\n       <input type=\"hidden\" name=\"mape\" value=\"428890.853892 4603274.55091 429500.453892 4603884.15091\">\\r\\n       <input type=\"hidden\" name=\"imgext\" value=\"428890.853892 4603274.55091 429500.453892 4603884.15091\">\\r\\n       <!--<input type=\"hidden\" name=\"mape\" value=\"423177 4513326 424910 4515010\">\\r\\n       <input type=\"hidden\" name=\"imgext\" value=\"423177 4513326 424910 4515010\">-->\\r\\n\\r\\n       <input type=\"hidden\" name=\"layer\" value=\"townrange\">\\r\\n       <input type=\"hidden\" name=\"layer\" value=\"wells2\">\\r\\n       <input type=\"hidden\" name=\"layer\" value=\"wells3\">\\r\\n       \\r\\n       <input type=\"hidden\" name=\"minx\" value=\"428890.853892\">\\r\\n       <input type=\"hidden\" name=\"maxx\" value=\"429500.453892\">\\r\\n       <input type=\"hidden\" name=\"miny\" value=\"4603274.55091\">\\r\\n       <input type=\"hidden\" name=\"maxy\" value=\"4603884.15091\">\\r\\n       \\r\\n       \\r\\n       <input type=\"hidden\" name=\"map_wells2_FILTER\" value=\"(\\'[WINLINK]\\' NE \\'\\' AND \\'[GEOL_LOG]\\' NE \\'1\\' AND ( \\'[WIN]\\' EQ \\'432043\\' ))\">\\r\\n       <input type=\"hidden\" name=\"map_wells2_Wells_EXPRESSION\" value=\"(\\'[WINLINK]\\' NE \\'\\' AND \\'[GEOL_LOG]\\' NE \\'1\\' AND ( \\'[WIN]\\' EQ \\'432043\\' ))\">\\r\\n       <input type=\"hidden\" name=\"map_wells3_FILTER\" value=\"(\\'[WINLINK]\\' NE \\'\\' AND \\'[GEOL_LOG]\\' EQ \\'1\\' AND ( \\'[WIN]\\' EQ \\'432043\\' ))\">\\r\\n       <input type=\"hidden\" name=\"map_wells3_Geologic_EXPRESSION\" value=\"(\\'[WINLINK]\\' NE \\'\\' AND \\'[GEOL_LOG]\\' EQ \\'1\\' AND ( \\'[WIN]\\' EQ \\'432043\\' ))\">\\r\\n       <!--<input type=\"hidden\" name=\"map_wells_Geologic Logs_EXPRESSION\" value=\"(\\'[GEOL_LOG]\\' EQ \\'1\\' AND ( \\'[WIN]\\' EQ \\'432043\\' ))\"-->\\r\\n       \\r\\n        \\r\\n       <input type=\"hidden\" name=\"layer\" value=\"doqs\">\\r\\n       \\r\\n       <input type=\"hidden\" name=\"zoomsize\" value=2>\\r\\n       \\r\\n       <input type=\"hidden\" name=\"program\" value=\"/cgi-bin/mapserv.exe\">\\r\\n       \\r\\n       <input type=\\'hidden\\' name=\\'map_web_imagepath\\' value=\\'d:/wrtdata/public/tmpdata/\\'>\\r\\n       \\r\\n       <input type=\\'hidden\\' name=\\'map_web_imageurl\\' value=\\'http://maps.waterrights.utah.gov/tmpdata/\\'>\\r\\n       \\r\\n       \\r\\n       <input type=\"hidden\" name=\"layer\" value=\"BOUNDARY\">\\r\\n       <input type=\"hidden\" name=\"map_BOUNDARY_feature\"  value=\"new\">\\r\\n       <input type=\"hidden\" name=\"map_BOUNDARY_feature_points\" value=\"429500.453892 4603579.35091 429499.294035978 4603605.91598039 429495.823295118 4603632.27887455 429490.068083853 4603658.23895495 429482.072202815 4603683.59864969 429471.896505489 4603708.16495618 429459.618435073 4603731.75091 429445.331435099 4603754.1770078 429429.144238262 4603775.27257343 429411.180038905 4603794.87705691 429391.575555432 4603812.84125626 429370.479989799 4603829.0284531 429348.053891999 4603843.31545307 429324.467938178 4603855.59352349 429299.901631685 4603865.76922082 429274.541936946 4603873.76510185 429248.581856551 4603879.52031312 429222.218962388 4603882.99105398 429195.653891998 4603884.15091 429169.088821609 4603882.99105398 429142.725927445 4603879.52031312 429116.765847051 4603873.76510185 429091.406152313 4603865.76922081 429066.83984582 4603855.59352349 429043.253891998 4603843.31545307 429020.827794198 4603829.0284531 428999.732228566 4603812.84125626 428980.127745093 4603794.8770569 428962.163545736 4603775.27257343 428945.976348899 4603754.1770078 428931.689348925 4603731.75091 428919.41127851 4603708.16495618 428909.235581183 4603683.59864968 428901.239700146 4603658.23895494 428895.484488881 4603632.27887455 428892.013748021 4603605.91598039 428890.853892 4603579.35091 428892.013748022 4603552.78583961 428895.484488882 4603526.42294544 428901.239700148 4603500.46286505 428909.235581186 4603475.10317031 428919.411278513 4603450.53686382 428931.689348928 4603426.95091 428945.976348903 4603404.5248122 428962.16354574 4603383.42924656 428980.127745097 4603363.82476309 428999.732228571 4603345.86056373 429020.827794203 4603329.6733669 429043.253892004 4603315.38636692 429066.839845825 4603303.10829651 429091.406152318 4603292.93259918 429116.765847057 4603284.93671815 429142.725927452 4603279.18150688 429169.088821615 4603275.71076602 429195.653892005 4603274.55091 429222.218962394 4603275.71076602 429248.581856558 4603279.18150688 429274.541936952 4603284.93671815 429299.90163169 4603292.93259919 429324.467938183 4603303.10829651 429348.053892004 4603315.38636693 429370.479989804 4603329.6733669 429391.575555437 4603345.86056374 429411.180038909 4603363.8247631 429429.144238266 4603383.42924657 429445.331435103 4603404.5248122 429459.618435076 4603426.95091 429471.896505491 4603450.53686383 429482.072202818 4603475.10317032 429490.068083854 4603500.46286506 429495.823295119 4603526.42294545 429499.294035979 4603552.78583962\">\\r\\n       \\r\\n       \\r\\n     </form>\\r\\n     <form name=\"googleform\" id=\"googleform\" method=\"POST\" action=\"http://maps.waterrights.utah.gov/py/wellsearch.py\" target=\\'_blank\\'>\\r\\n            \\r\\n       <input type=\"hidden\" name=\"wins\" value=\"432043\">\\r\\n       <input type=\"hidden\" name=\"SearchRadius\" value=\"1000\">\\r\\n       <input type=\"hidden\" name=\"xutm\" value=\"429195.653892\">\\r\\n       <input type=\"hidden\" name=\"yutm\" value=\"4603579.35091\">\\r\\n       \\r\\n       <input type=\"hidden\" name=\"Section\" value=\"EntireTownship\">\\r\\n       <input type=\"hidden\" name=\"Township\" value=\"1N\">\\r\\n       <input type=\"hidden\" name=\"Range\" value=\"1E\">\\r\\n       <input type=\"hidden\" name=\"BM\" value=\"SL\">\\r\\n       <input type=\"hidden\" name=\"mOption\" value=\"radius\">\\r\\n         \\r\\n     </form>\\r\\n     \\r\\n                     \\t\\t\\r\\n  \\t\\r\\n  <script type=\"text/javascript\">\\r\\n    function startup() {\\r\\n\\r\\n    }\\r\\n    function viewMap() {\\r\\n    \\t//mapform.submit()\\r\\n    \\tmapformfrm=document.getElementById(\"mapform\");\\r\\n    \\tmapformfrm.submit();\\r\\n    }\\r\\n    function viewGoogleMap() {\\r\\n    \\t//googleform.submit()\\r\\n    \\tgoogleformfrm=document.getElementById(\"googleform\");\\r\\n    \\tgoogleformfrm.submit();\\r\\n    }\\r\\n  </script>\\r\\n\\r\\n\\r\\n</td>\\r\\n</tr>\\r\\n</table>\\r\\n<script language=\"javascript\" src=\"http://waterrights.utah.gov/footer.js\" type=\"text/javascript\"></script>\\r\\n</body>'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winmatch(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://wwwsearch.sourceforge.net/mechanize/forms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def winmatch(x):\n",
    "    request = mechanize.Request(\"http://waterrights.utah.gov/wellinfo/wellsearch.asp\")\n",
    "    response = mechanize.urlopen(request)\n",
    "    forms = mechanize.ParseResponse(response, backwards_compat=False)\n",
    "    response.close()\n",
    "    form = forms[0] \n",
    "    #print form\n",
    "    form[\"mainoption\"]=[\"radius\"]\n",
    "    form[\"SearchRadius\"]=\"1000\"\n",
    "    form[\"option\"]=[\"UTM\"]\n",
    "    form[\"xUTM\"]=str(x[0])\n",
    "    form[\"yUTM\"]=str(x[1])\n",
    "    if len(siteinfo.construction_dt)>=4:\n",
    "        form[\"DateRange\"]=[\"on\"]\n",
    "        form[\"edtDrillBef\"]=str(int(str(x[2])[:4])+5)\n",
    "        form[\"edtDrillAft\"]=str(int(str(x[2])[:4])-5)\n",
    "        print str(int(str(x[2])[:4]))\n",
    "    if len(siteinfo.well_depth_va)>0:\n",
    "        form[\"DepthRange\"]=[\"on\"]\n",
    "        form[\"edtDrillBel\"]=str(int(x[3])-10)\n",
    "        form[\"edtDrillAbo\"]=str(int(x[3])+30)\n",
    "        print str(str(int(x[3])))\n",
    "    desc =  mechanize.urlopen(form.click()).read()\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siteinfo['Elev'] = siteinfo[['dec_long_va','dec_lat_va']].apply(lambda x: getelev(x),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siteinfo['USGSid'] = siteinfo[['dec_long_va','dec_lat_va']].apply(lambda x: USGSID(x),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N89 E1320 S4 31 12N 1E SL\n"
     ]
    }
   ],
   "source": [
    "siteinfo['PLSS'], siteinfo['CAD'] = zip(*siteinfo[['UTM_X','UTM_Y']].apply(lambda x: getwellinfo(x),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siteinfo.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datahtml = \"http://waterservices.usgs.gov/nwis/gwlevels/?format=rdb&sites=\"+stations+\"&startDT=1800-01-01&endDT=\"+str(datetime.today().year)+\"-\"+str(datetime.today().month).zfill(2)+\"-\"+str(datetime.today().day).zfill(2)\n",
    "response = urllib2.urlopen(datahtml)\n",
    "html = response.read()\n",
    "skip = html[:html.rfind('#\\n')+2].count('\\n')\n",
    "skiplist = range(0,skip)\n",
    "skiplist.append(skip+1)\n",
    "data = pd.read_table(datahtml, sep=\"\\t\",skiprows=skiplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.drop([u'agency_cd', u'site_tp_cd'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stationWL = pd.merge(data, siteinfo, on='site_no', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NAD83'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationWL['dec_coord_datum_cd'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stationWL = stationWL[~stationWL['lev_status_cd'].isin(['Z', 'R', 'V', 'P', 'O', 'F', 'W', 'G', 'S', 'C', 'E', 'N'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stationWL['wlelev'] = stationWL[['lev_va','Elev']].apply(lambda x: getwlelev(x),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = stationWL.wlelev.values\n",
    "x = stationWL.UTM_X.values\n",
    "y = stationWL.UTM_Y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://connor-johnson.com/2014/03/20/simple-kriging-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stackoverflow.com/questions/31124930/ckdtree-vs-dsearchn?lq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "A = np.loadtxt('A.txt')\n",
    "B = np.loadtxt('B.txt')\n",
    "tree = cKDTree( B[:,[1,2,3]] )\n",
    "d, inds = tree.query( A[:,[1,2,3]], k=1, p=2)\n",
    "B_new = B[inds]\n",
    "xyz_near = np.hstack(( B_new[:,0:4], A[:,0:4] ))\n",
    "\n",
    "for j, a in enumerate(A):\n",
    "    # compute 2-norms from each point in B to a\n",
    "    dd = np.sqrt(((a[1:] - B[:,1:])**2).sum(axis=1))\n",
    "    # find closest point\n",
    "    jx = np.argmin(dd)\n",
    "    # check solution\n",
    "    assert inds[j] == jx\n",
    "    assert np.allclose(d[j], dd.min())\n",
    "    # check it is unique\n",
    "    assert (dd[jx+1:] > d[j]).all()\n",
    "    assert (dd[:jx] > d[j]).all()\n",
    "\n",
    "print(\"All OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-a9052b19ea44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0myi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mymin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mny\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Interpolate the values of z for all points in the rectangular grid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36mmeshgrid\u001b[1;34m(*xi, **kwargs)\u001b[0m\n\u001b[0;32m   3104\u001b[0m         \u001b[1;31m# Return the full N-D matrix (not only the 1-D vector)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3106\u001b[1;33m             \u001b[0mmult_fact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3107\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmult_fact\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36mones\u001b[1;34m(shape, dtype, order)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \"\"\"\n\u001b[1;32m--> 178\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "__date__    2013-11-16\n",
    "__author__  josef \n",
    "__web__     http://hydrogeotools.blogspot.se/\n",
    "To import xyz data from an ascii file, interpolate and save as geotiff\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.mlab as ml\n",
    "import scipy.interpolate as il #for method2, in case the matplotlib griddata method fails\n",
    "from osgeo import gdal\n",
    "from osgeo import osr\n",
    "\n",
    "\n",
    "z = stationWL.wlelev.values\n",
    "x = stationWL.UTM_X.values\n",
    "y = stationWL.UTM_Y.values\n",
    "\n",
    "\n",
    "xmin,xmax,ymin,ymax = [min(x),max(x),min(y),max(y)]\n",
    "\n",
    "#size of 1 m grid\n",
    "nx = (int(xmax - xmin + 1))#CHANGE HERE\n",
    "ny = (int(ymax - ymin + 1))#CHANGE HERE\n",
    "\n",
    "# Generate a regular grid to interpolate the data.\n",
    "xi = np.linspace(xmin, xmax, nx)\n",
    "yi = np.linspace(ymin, ymax, ny)\n",
    "xi, yi = np.meshgrid(xi, yi) \n",
    " \n",
    "# Interpolate the values of z for all points in the rectangular grid\n",
    "# Method 1 - Interpolate by matplotlib delaunay triangularizatio and nearest neigh. PLEASE NOTE! THIS FAILS QUITE OFTEN (http://matplotlib.org/api/mlab_api.html#matplotlib.mlab.griddata) But there might be a solution - install mpl_toolkits.natgrid (http://matplotlib.org/mpl_toolkits/)\n",
    "zi = ml.griddata(x,y,z,xi,yi,interp='nn') #interpolation is 'nn' by default (natural neighbour based on delaunay triangulation) but 'linear' is faster (see http://matplotlib.1069221.n5.nabble.com/speeding-up-griddata-td20906.html)\n",
    "# PLEASE NOTE! Method 1 fails sometimes and then using mpl_toolkits.natgrid may be a solution (http://matplotlib.org/api/mlab_api.html#matplotlib.mlab.griddata) (http://matplotlib.org/mpl_toolkits/)\n",
    " \n",
    "# Otherwise, try Method 2 - Interpolate  using scipy interpolate griddata\n",
    "zi = il.griddata((x, y), z, (xi, yi),method='linear') #(may use 'nearest', 'linear' or 'cubic'  - although constant problems w linear) \n",
    " \n",
    "#---------------  Write to GeoTIFF ------------------------\n",
    "nrows,ncols = np.shape(zi)\n",
    "xres = (xmax-xmin)/float(ncols)\n",
    "yres = (ymax-ymin)/float(nrows)\n",
    "geotransform=(xmin,xres,0,ymin,0, yres) \n",
    " \n",
    "output_raster = gdal.GetDriverByName('GTiff').Create(raster_ut,ncols, nrows, 1 ,gdal.GDT_Float32,['TFW=YES', 'COMPRESS=PACKBITS'])  # Open the file, see here for information about compression: http://gis.stackexchange.com/questions/1104/should-gdal-be-set-to-produce-geotiff-files-with-compression-which-algorithm-sh\n",
    "output_raster.SetGeoTransform(geotransform)  # Specify its coordinates\n",
    "srs = osr.SpatialReference()                 # Establish its coordinate encoding\n",
    "srs.ImportFromEPSG(3010)                     # This one specifies SWEREF99 16 30\n",
    "output_raster.SetProjection( srs.ExportToWkt() )   # Exports the coordinate system to the file\n",
    "output_raster.GetRasterBand(1).WriteArray(zi)   # Writes my array to the raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script will generate a series of box and whisker plots and save them in a pdf. It makes a box plot for each station, breaking the data into monthly intervals.  Make sure to change the directory name in the script so it ends up in a recognizable place on your computer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dictionary of integers and their month equivalent\n",
    "months = {'1':'Jan.', '2':'Feb.', '3':'Mar.', '4':'Apr.', '5':'May', '6':'Jun.', \n",
    "         '7':'Jul.', '8':'Aug.', '9':'Sep.', '10':'Oct.', '11':'Nov.', '12':'Dec.', 'Total':'Total'}\n",
    "# create empty dictionary to hold pandas Dataframes\n",
    "j = {}\n",
    "\n",
    "\n",
    "with PdfPages(rootname + 'station_boxplots.pdf') as pdfs:\n",
    "    ymax = 10000\n",
    "    ymin = 0.01\n",
    "    for i in range(len(sites)):\n",
    "        # make a dataframe containing summary statistics and store it in the j dictionary\n",
    "        j[sites[i]] = USGS_Site_Data.groupby('mon')[sites[i]].agg({'min':np.min, 'mean':np.mean, \n",
    "                                                                   'median':np.median, 'max':np.max, 'std':np.std, \n",
    "                                                                   'cnt':(lambda x: np.count_nonzero(~np.isnan(x)))}).reset_index()\n",
    "        # make a list of the custom lables you will use for your boxplot; this one will show the number of samples used to make the plot\n",
    "        labs = [months[(str(j[sites[i]]['mon'][b]))] + \" (n=\" + str(int(j[sites[i]]['cnt'][b])) + \")\" for b in range(len(j[sites[i]]))]\n",
    "        # designate the location of each custom label\n",
    "        tickloc = [b+1 for b in range(len(j[sites[i]]['mon']))]\n",
    "        \n",
    "        plt.figure()\n",
    "        USGS_Site_Data.boxplot(column=sites[i],by='mon', rot=70)\n",
    "        strtdt = str(USGS_Site_Info.ix[sites[i],'start_date'])[0:10]\n",
    "        findt = str(USGS_Site_Info.ix[sites[i],'fin_date'])[0:10]\n",
    "        siteName = USGS_Site_Info.ix[sites[i],'name'].title() \n",
    "        plt.title( siteName + ' (' + sites[i] + ')  ' + strtdt + ' to ' + findt )\n",
    "        plt.suptitle('')\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Discharge (cfs)')\n",
    "        plt.ylim((ymin,ymax))\n",
    "        plt.xlabel('Month')\n",
    "        # here is where your lists for the custom label come into play\n",
    "        plt.xticks(tickloc, labs)\n",
    "        \n",
    "        pdfs.savefig()\n",
    "\n",
    "        plt.close()\n",
    "    # Save metadata of the pdf so you can find it later\n",
    "    d = pdfs.infodict()\n",
    "    d['Title'] = 'Monthly Station USGS Boxplots UMSS'\n",
    "    d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "    d['Subject'] = 'Boxplots of several USGS Surface Stations'\n",
    "    d['Keywords'] = 'USGS Surface NWIS Boxplot'\n",
    "    d['CreationDate'] = datetime.today()\n",
    "    d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few of the boxplots so you can see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,3):\n",
    "    j[sites[i]] = USGS_Site_Data.groupby('mon')[sites[i]].agg([np.min, np.mean, np.median, np.max, np.std, np.size]).reset_index()\n",
    "    # make a list of the custom lables you will use for your boxplot; this one will show the number of samples used to make the plot\n",
    "    labs = [months[(str(j[sites[i]]['mon'][b]))] + \" (n=\" + str(int(j[sites[i]]['size'][b])) + \")\" for b in range(len(j[sites[i]]))]\n",
    "    # designate the location of each custom label\n",
    "    tickloc = [b+1 for b in range(len(j[sites[i]]['mon']))]\n",
    "\n",
    "    plt.figure()\n",
    "    USGS_Site_Data.boxplot(column=sites[i],by='mon', rot=70)\n",
    "    plt.title(USGS_Site_Info.ix[sites[i],'name'].title() + ' (' + sites[i] + ')')\n",
    "    plt.suptitle('')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('Discharge (cfs)')\n",
    "    plt.ylim((ymin,ymax))\n",
    "    plt.xlabel('Month')\n",
    "    # here is where your lists for the custom label come into play\n",
    "    plt.xticks(tickloc, labs)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will generate boxplots showing all of the station data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This script summarizes discharge for all sites and limits the number of box plots on one graph to the n variable\n",
    "j=0\n",
    "with PdfPages(rootname + 'sum_boxplots.pdf') as pdf:\n",
    "    while j < len(sites):\n",
    "        ymax = 10000\n",
    "        ymin = 0.01\n",
    "        n=10\n",
    "        # if statement allows for uneven number of sites on last page\n",
    "        if j+n >= len(sites):\n",
    "            plt.figure()\n",
    "            USGS_Site_Data[sites[j:-1]].plot(kind='box')\n",
    "            plt.title('Sites '+sites[j]+' to '+sites[-1] )\n",
    "            plt.yscale('log')\n",
    "            plt.xlabel('USGS Site')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel('discharge (cfs)')\n",
    "            plt.ylim((ymin,ymax))\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            j = j+n\n",
    "        else:\n",
    "            plt.figure()\n",
    "            USGS_Site_Data[sites[j:j+n]].plot(kind='box')\n",
    "            plt.title('Sites '+sites[j]+' to '+sites[j+n] )\n",
    "            plt.yscale('log')\n",
    "            plt.xlabel('USGS Site')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel('discharge (cfs)')\n",
    "            plt.ylim((ymin,ymax))\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            j = j+n\n",
    "        # Save metadata of the pdf so you can find it later\n",
    "        d = pdf.infodict()\n",
    "        d['Title'] = 'Summary USGS Boxplots UMSS'\n",
    "        d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "        d['Subject'] = 'Boxplots of several USGS Surface Stations'\n",
    "        d['Keywords'] = 'USGS Surface NWIS Boxplot'\n",
    "        d['CreationDate'] = datetime.today()\n",
    "        d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also produce hydrographs of each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xmax = USGS_Site_Data.index.astype(datetime).values[-1]\n",
    "xmin = USGS_Site_Data.index.astype(datetime).values[0]\n",
    "\n",
    "pdfs = PdfPages(rootname + 'station_hydrographs.pdf')\n",
    "ymax = 10000\n",
    "ymin = 0.1\n",
    "for i in range(len(sites)):\n",
    "    x = USGS_Site_Data.index.values\n",
    "    y = USGS_Site_Data[sites[i]].values\n",
    "    plt.figure()\n",
    "    plt.plot(x,y)\n",
    "    strtdt = str(USGS_Site_Info.ix[sites[i],'start_date'])[0:10]\n",
    "    findt = str(USGS_Site_Info.ix[sites[i],'fin_date'])[0:10]\n",
    "    siteName = USGS_Site_Info.ix[sites[i],'name'].title() \n",
    "    plt.title( siteName + ' (' + sites[i] + ')  ' + strtdt + ' to ' + findt )\n",
    "    plt.suptitle('')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('Discharge (cfs)')\n",
    "    plt.ylim((ymin,ymax))\n",
    "    plt.xlabel('Year')\n",
    "    plt.xticks(np.arange(datetime(1905,1,1),xmax+timedelta(days=365.25),timedelta(days=365.25*5)),rotation=45)\n",
    "    plt.xlim(xmin,xmax)\n",
    "    pdfs.savefig()\n",
    "    plt.close()\n",
    "    # Save metadata of the pdf so you can find it later\n",
    "\n",
    "d = pdfs.infodict()\n",
    "d['Title'] = 'Monthly Station USGS Hydrographs UMSS'\n",
    "d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "d['Subject'] = 'Hydrograph of several USGS Surface Stations'\n",
    "d['Keywords'] = 'USGS Surface NWIS Hydrograph'\n",
    "d['CreationDate'] = datetime.today()\n",
    "d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.date_range(start=xmin, end=xmax, freq='5AS').year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmax = USGS_Site_Data.index[-1]\n",
    "xmin = USGS_Site_Data.index[0]\n",
    "\n",
    "plt.figure()\n",
    "ticks = pd.date_range(start=xmin, end=xmax, freq='4AS')\n",
    "USGS_Site_Data[sites[0:3]].plot(subplots=True,sharex=True,figsize=(10,8),logy=True, rot=90)\n",
    "plt.xlim(xmin,xmax)\n",
    "labs = pd.date_range(start=xmin, end=xmax, freq='4AS').year\n",
    "plt.xticks(ticks,labs)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lumped_hydro(i1,i2):\n",
    "    pdfs = PdfPages(rootname + 'station_hydrographs_lumped.pdf')\n",
    "    plt.figure()\n",
    "    ticks = pd.date_range(start=xmin, end=xmax, freq='4AS')\n",
    "    USGS_Site_Data[sites[i1:i2]].plot(subplots=True, sharex=True, figsize=(10,24),logy=True, rot=90)\n",
    "    plt.xlim(xmin,xmax)\n",
    "    labs = pd.date_range(start=xmin, end=xmax, freq='4AS').year\n",
    "    plt.xticks(ticks,labs)\n",
    "    pdfs.savefig()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lumped_hydro(0,10)\n",
    "lumped_hydro(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lumped_hydro(20,30)\n",
    "lumped_hydro(30,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will iteratively produce Flow Duration Curves for each of the stations. A flow duration curve is a <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#PPF>percent point function (ppf)</a>, displaying discharge as a function of probability of that discharge occuring. The ppf is the inverse of the better known <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#CDF>cumulative distribution function (cdf)</a>. See <a href=http://pubs.usgs.gov/wsp/1542a/report.pdf>this USGS publication</a> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages(rootname+'station_fdc.pdf') as pdf:\n",
    "    ymax = 10000\n",
    "    ymin = 0.01\n",
    "    for i in range(len(sites)):\n",
    "        plt.figure()\n",
    "        fdc_simple(USGS_Site_Data,sites[i],1900,2015)\n",
    "        fdc_simple(USGS_Site_Data,sites[i],1900,1970)\n",
    "        fdc_simple(USGS_Site_Data,sites[i],1970,2015)\n",
    "        plt.ylim(0.01,10000)\n",
    "        plt.xlim(-.05,1.05)\n",
    "        plt.grid(which = 'both')\n",
    "        plt.legend()\n",
    "        plt.xlabel('probability that discharge was exceeded or equaled')\n",
    "        plt.title('Flow duration curve for ' + str(USGS_Site_Info['name'][i]).title() + ' ('+ sites[i] +')'+'\\n'+\n",
    "                  'Record: ' + str(USGS_Site_Info['start_date'][i])[0:10] + ' to ' + str(USGS_Site_Info['fin_date'][i])[0:10])\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('discharge (cfs)')\n",
    "        plt.xticks(np.arange(0,1.05,0.05))\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "    # Save metadata of the pdf so you can find it later\n",
    "    d = pdf.infodict()\n",
    "    d['Title'] = 'Flow Duration Curves USGS'\n",
    "    d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "    d['Subject'] = 'Flow Duration Curves of several USGS Surface Stations'\n",
    "    d['Keywords'] = 'USGS Surface NWIS FDC Flow Duration'\n",
    "    d['CreationDate'] = datetime.today()\n",
    "    d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For brevity, here is an example plot from the output of the script above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fdc_simple(USGS_Site_Data,sites[38],1900,2015)\n",
    "fdc_simple(USGS_Site_Data,sites[38],1900,1970)\n",
    "fdc_simple(USGS_Site_Data,sites[38],1970,2015)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid(which = 'both')\n",
    "plt.xlabel('% of time that indicated discharge was exceeded or equaled')\n",
    "plt.ylabel('discharge (cfs)')\n",
    "plt.xticks(np.arange(0.0,1.05,0.05))\n",
    "plt.title('Flow duration curve for ' + str(USGS_Site_Info['name'][38]) + ' ('+ sites[i] +')'+'\\n'+ \n",
    "          'Record: ' + str(USGS_Site_Info['start_date'][i])[0:10] + ' to ' + str(USGS_Site_Info['fin_date'][i])[0:10])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some sad attempts to model the flow duration curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site = sites[28]\n",
    "df = USGS_Site_Data[[site]]\n",
    "begyear = 1900\n",
    "endyear = 2015\n",
    "data = df[(df.index.to_datetime() > pd.datetime(begyear,1,1))&(df.index.to_datetime() < pd.datetime(endyear,1,1))]\n",
    "data = data.dropna()\n",
    "\n",
    "data['doy']=data.index.dayofyear\n",
    "dailyavg = data[site].groupby(data['doy']).mean()\n",
    "\n",
    "\n",
    "data = np.sort(dailyavg)\n",
    "\n",
    "mean = np.mean(data)\n",
    "std = np.std(data)\n",
    "f = [(data[i]) for i in range(len(data))]\n",
    "#f = [(data[i])/mean for i in range(len(data))]\n",
    "#f = [(data[i]-std)/mean for i in range(len(data))]\n",
    "\n",
    "ranks = sp.rankdata(f, method='average')\n",
    "ranks = ranks[::-1]\n",
    "prob = [(ranks[i]/(len(f)+1)) for i in range(len(f)) ]\n",
    "\n",
    "x = prob\n",
    "y = f\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y,x,label=site,color='blue')\n",
    "#plt.yscale('log')\n",
    "#plt.xlim(-.05,1.05)\n",
    "plt.grid(which = 'both')\n",
    "plt.ylabel('probability that discharge was exceeded or equaled')\n",
    "plt.xlabel('normalized discharge')\n",
    "#plt.xticks(np.arange(0,1.00,0.05))\n",
    "plt.title('Flow duration curve for ' + site + ' averaged from ' + str(begyear) + ' to ' + str(endyear))\n",
    "\n",
    "def func(x,a,b,c,d):\n",
    "    return a*np.log(x*b+c)+d\n",
    "\n",
    "par, cov = op.curve_fit(func,y,x,p0=[-0.16528617, 1.54535185, -24.70440088, 0.9])\n",
    "plt.plot(y, [par[0]*np.log(y[i]*par[1]+par[2])+par[3] for i in range(len(y))], color='red')\n",
    "print 'curve fit', sp.linregress(x,[par[0]*np.log(y[i]*par[1]+par[2])+par[3] for i in range(len(y))])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x,y,label=site,color='blue')\n",
    "#plt.yscale('log')\n",
    "#plt.xlim(-.05,1.05)\n",
    "plt.grid(which = 'both')\n",
    "plt.xlabel('probability that discharge was exceeded or equaled')\n",
    "plt.ylabel('normalized discharge')\n",
    "#plt.xticks(np.arange(0,1.00,0.05))\n",
    "plt.title('Flow duration curve for ' + site + ' averaged from ' + str(begyear) + ' to ' + str(endyear))\n",
    "\n",
    "def func2(x,a,b,c,d):\n",
    "    return a*np.exp(x*b+c)+d\n",
    "\n",
    "\n",
    "parm, covm = op.curve_fit(func2,x,y,p0=[-0.16528617, 0.02, 0.70440088, 0.9])\n",
    "plt.plot(x, [parm[0]*np.exp(x[i]*parm[1]+parm[2])+parm[3] for i in range(len(x))], color='red')\n",
    "print 'curve fit', sp.linregress(y,[parm[0]*np.exp(x[i]*parm[1]+parm[2])+parm[3] for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dic2df(dic,head):\n",
    "    df = pd.DataFrame(data=dic)\n",
    "    df = df.transpose()\n",
    "    df.columns = [str(head)+'_var1',str(head)+'_var2',str(head)+'_var3',str(head)+'_var4',str(head)+'_r2',str(head)+'_err']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = {}; m = {}; n = {}; k = {}; j = {}; p = {}\n",
    "\n",
    "for site in sites:\n",
    "    q[site] = fdcmatch(USGS_Site_Data,site,1900,2015,1,1)\n",
    "    m[site] = fdcmatch(USGS_Site_Data,site,1900,1970,1,1)\n",
    "    n[site] = fdcmatch(USGS_Site_Data,site,1970,2015,1,1)\n",
    "    k[site] = fdcmatch(USGS_Site_Data,site,1900,2015,1,0)\n",
    "    j[site] = fdcmatch(USGS_Site_Data,site,1900,1970,1,0)\n",
    "    p[site] = fdcmatch(USGS_Site_Data,site,1970,2015,1,0)\n",
    "    \n",
    "dics = [q,m,n,k,j,p]\n",
    "heads = ['all','to70','fm70','allin','to70in','fm70in']\n",
    "\n",
    "USGS_q = dic2df(q,'all')\n",
    "USGS_m = dic2df(m,'to70')\n",
    "USGS_parms = pd.merge(USGS_q,USGS_m, left_index=True, right_index=True, how='outer' )\n",
    "\n",
    "for i in range(2,6,1):\n",
    "    x = dic2df(dics[i],heads[i])\n",
    "    USGS_parms = pd.merge(USGS_parms,x, left_index=True, right_index=True, how='outer' )\n",
    "\n",
    "USGS_parms.to_clipboard()\n",
    "#USGS_finish_date = USGS_finish_date.drop([0],axis=1)\n",
    "#USGS_start_fin = pd.merge(USGS_finish_date,USGS_start_date, left_index=True, right_index=True, how='outer' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations from the modeled fdc plots can be used to estimate the discharge for a site based on data from a similar site.  However, the results are mediocre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n1 = 12\n",
    "n2 = 11\n",
    "\n",
    "USGS_Site_Data[sites[n1]+'p'] = [USGS_parms['all_var1'][n1]*np.log(USGS_Site_Data[sites[n1]][i]*USGS_parms['all_var2'][n1]+\\\n",
    "                                                                   USGS_parms['all_var3'][n1])+USGS_parms['all_var4'][n1] for i in \\\n",
    "range(len(USGS_Site_Data[sites[n1]]))]\n",
    "\n",
    "USGS_Data = USGS_Site_Data[USGS_Site_Data[sites[n1]+'p']>0]\n",
    "\n",
    "USGS_Data[sites[n2]+'d'] = [USGS_parms['allin_var1'][n2]*np.exp(USGS_Data[sites[n1]+'p'][i]*USGS_parms['allin_var2'][n2]+\\\n",
    "                                                                   USGS_parms['allin_var3'][n2])+USGS_parms['allin_var4'][n2] for i in \\\n",
    "range(len(USGS_Data[sites[n1]+'p']))]\n",
    "\n",
    "y1 = USGS_Data[sites[n2]+'d'].values\n",
    "y2 = USGS_Site_Data[sites[n2]].values\n",
    "\n",
    "x1 = USGS_Data.index.to_datetime()\n",
    "x2 = USGS_Site_Data.index.to_datetime()\n",
    "y3 = USGS_Data[sites[n1]]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x1,y1, label=\"modeled discharge\")\n",
    "plt.plot(x2,y2, label=\"actual discharge\")\n",
    "plt.plot(x1,y3, label=\"reference discharge\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "#plt.xlim(USGS_Site_Info['start_date'][n2],USGS_Site_Info['fin_date'][n2])\n",
    "plt.xlim('1/1/1970','1/1/1974')\n",
    "#plt.ylim(1,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stackoverflow.com/questions/15408371/cumulative-distribution-plots-python <br>\n",
    "http://hydroclimpy.sourceforge.net/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following script if you want to see a map of your stations.  This assumes that you have the <a href=http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/>Basemap package</a> installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "X = USGS_Site_Info['longitude'].astype(float).values.tolist()\n",
    "Y = USGS_Site_Info['latitude'].astype(float).values.tolist()\n",
    "\n",
    "n = 0.05 \n",
    "m = Basemap(llcrnrlon=min(X)+n,llcrnrlat=min(Y)+n,urcrnrlon=max(X)+n,urcrnrlat=max(Y)+n,\n",
    "            resolution='h',projection='cyl',lon_0=np.mean(X),lat_0=np.mean(Y))\n",
    "m.drawrivers(color='blue',linewidth=0.5)\n",
    "m.drawcounties(color='red',linewidth=0.5)\n",
    "m.arcgisimage()\n",
    "#m.etopo(scale=0.5)\n",
    "lons = X\n",
    "lats = Y\n",
    "x,y = m(lons,lats)\n",
    "m.plot(x,y,'ro', markersize=8)\n",
    "\n",
    "#m.drawmapscale(lon=-114, lat=43.5, length=100, lon0=-114, lat0=39, barstyle='simple', units='km')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col = USGS_Site_Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects as robj\n",
    "import rpy2.rlike.container as rlc\n",
    "\n",
    "def pandas_data_frame_to_rpy2_data_frame(pDataframe):\n",
    "    orderedDict = rlc.OrdDict()\n",
    "\n",
    "    for columnName in pDataframe:\n",
    "        columnValues = pDataframe[columnName].values\n",
    "        filteredValues = [value if pd.notnull(value) else robj.NA_Real \n",
    "                          for value in columnValues]\n",
    "\n",
    "        try:\n",
    "            orderedDict[columnName] = robj.FloatVector(filteredValues)\n",
    "        except ValueError:\n",
    "            orderedDict[columnName] = robj.StrVector(filteredValues)\n",
    "\n",
    "    rDataFrame = robj.DataFrame(orderedDict)\n",
    "    rDataFrame.rownames = robj.StrVector(pDataframe.index)\n",
    "\n",
    "    return rDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USD = pandas_data_frame_to_rpy2_data_frame(USGS_Site_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
