{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font size=\"5\">Using Flow Duration Curves to Determine Basin Characteristics and Estimate Flow</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the Scientific Python (scipy) stack tools to generate flow duration curves from current USGS NWIS data.\n",
    "\n",
    "Using recipes from this notebook, you can make:\n",
    "* USGS Station Summaries\n",
    "* Flow duration curves\n",
    "* Iterative import and compilation of USGS station information and data\n",
    "* boxplots using pandas\n",
    "* iterative charts (one monthly summary boxplot per station)\n",
    "* Gantt charts of USGS stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages. <a href=http://ulmo.readthedocs.org/en/latest/>`ulmo`</a> is not a standard package and will have to be loaded into your local python repository for some of these functions to work. Use the following command in your cmd prompt to install `ulmo`.\n",
    "```cmd\n",
    "pip install ulmo\n",
    "```\n",
    "You might also be missing the <a href=http://pandas.pydata.org/>`pandas`</a> and <a href=http://www.scipy.org/>`scipy`</a> packages, which is a shame, because `pandas` is awesome.\n",
    "```cmd\n",
    "pip install pandas\n",
    "pip install scipy\n",
    "```\n",
    "Check out this for some great `pandas` applications:\n",
    "http://earthpy.org/time_series_analysis_with_pandas_part_2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "import scipy.stats as sp\n",
    "import scipy.optimize as op\n",
    "import statsmodels.api as sm\n",
    "import ulmo\n",
    "from pandas.stats.api import ols\n",
    "from datetime import datetime, date, timedelta\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.pyplot import cm \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger('ulmo.usgs.nwis.core')\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rpy2` allows one to run `R` files within IPython Notebook. You may need to configure the environment variable settings for this to work properly. If you get errors, use the Google. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import rpy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a directory to store output figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rootname = 'U:\\\\GWP\\\\Groundwater\\\\UMSS_Manti\\\\Data\\\\Hydrology\\\\plots\\\\'\n",
    "dataroot = 'U:\\\\GWP\\\\Groundwater\\\\UMSS_Manti\\\\Data\\\\Hydrology\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `importusgssite` uses the <a href=http://ulmo.readthedocs.org/en/latest/>`ulmo`</a> package to retrieve U.S. Geological Survey surface water site data from the <a href=http://waterdata.usgs.gov/nwis>National Water Information System (NWIS) website</a>.  The function also puts the data from the website into a usable format. `getusgssiteinfo` gets site information from NWIS, which includes site name, watershed size, and coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def importusgssite(siteno):\n",
    "    sitename = {}\n",
    "    sitename = ulmo.usgs.nwis.get_site_data(siteno, service=\"daily\", period=\"all\")\n",
    "    sitename = pd.DataFrame(sitename['00060:00003']['values'])\n",
    "    sitename['dates'] = pd.to_datetime(pd.Series(sitename['datetime']))\n",
    "    sitename.set_index(['dates'],inplace=True)\n",
    "    sitename[siteno] = sitename['value'].astype(float)\n",
    "    sitename[str(siteno)+'qual'] = sitename['qualifiers']\n",
    "    sitename = sitename.drop(['datetime','qualifiers','value'],axis=1)\n",
    "    sitename = sitename.replace('-999999',np.NAN)\n",
    "    sitename = sitename.dropna()\n",
    "    #sitename['mon']=sitename.index.month\n",
    "    return sitename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getusgssiteinfo(siteno):\n",
    "    siteinfo = ulmo.usgs.nwis.get_site_data(siteno, service=\"daily\", period=\"all\")\n",
    "    siteinfo = pd.DataFrame(siteinfo['00060:00003']['site'])\n",
    "    siteinfo['latitude'] = siteinfo.loc['latitude','location']\n",
    "    siteinfo['longitude'] = siteinfo.loc['longitude','location']\n",
    "    siteinfo['latitude'] = siteinfo['latitude'].astype(float)\n",
    "    siteinfo['longitude'] = siteinfo['longitude'].astype(float)\n",
    "    siteinfo = siteinfo.drop(['default_tz','dst_tz','srs','uses_dst','longitude'],axis=0)\n",
    "    siteinfo = siteinfo.drop(['agency','timezone_info','location','state_code','network'],axis=1)\n",
    "    return siteinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fdc` generates a flow duration curve for hydrologic data. A flow duration curve is a <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#PPF>percent point function (ppf)</a>, displaying discharge as a function of probability of that discharge occuring. The ppf is the inverse of the better known <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#CDF>cumulative distribution function (cdf)</a>. See <a href=http://pubs.usgs.gov/wsp/1542a/report.pdf>this USGS publication</a> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fdc(df,site,begyear,endyear):\n",
    "    '''\n",
    "    Generate flow duration curve for hydrologic time series data\n",
    "    \n",
    "    df = pandas dataframe containing data\n",
    "    site = column within dataframe that contains the flow values\n",
    "    begyear = start year for analysis\n",
    "    endyear = end year for analysis\n",
    "    '''\n",
    "        \n",
    "    data = df[(df.index.to_datetime() > pd.datetime(begyear,1,1))&(df.index.to_datetime() < pd.datetime(endyear,1,1))]\n",
    "    data = data[site].dropna().values\n",
    "    data = np.sort(data)\n",
    "    ranks = sp.rankdata(data, method='average')\n",
    "    ranks = ranks[::-1]\n",
    "    prob = [100*(ranks[i]/(len(data)+1)) for i in range(len(data)) ]\n",
    "    plt.figure()\n",
    "    plt.scatter(prob,data,label=site)\n",
    "    plt.yscale('log')\n",
    "    plt.grid(which = 'both')\n",
    "    plt.xlabel('% of time that indicated discharge was exceeded or equaled')\n",
    "    plt.ylabel('discharge (cfs)')\n",
    "    plt.xticks(range(0,100,5))\n",
    "    plt.title('Flow duration curve for ' + site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fdc_simple(df, site, begyear=1900, endyear=2015, normalizer=1):\n",
    "    '''\n",
    "    Generate flow duration curve for hydrologic time series data\n",
    "    \n",
    "    PARAMETERS:\n",
    "        df = pandas dataframe of interest; must have a date or date-time as the index\n",
    "        site = pandas column containing discharge data; must be within df\n",
    "        begyear = beginning year of analysis; defaults to 1900\n",
    "        endyear = end year of analysis; defaults to 2015\n",
    "        normalizer = value to use to normalize discharge; defaults to 1 (no normalization)\n",
    "    \n",
    "    RETURNS:\n",
    "        matplotlib plot displaying the flow duration curve of the data\n",
    "        \n",
    "    REQUIRES:\n",
    "        numpy as np\n",
    "        pandas as pd\n",
    "        matplotlib.pyplot as plt\n",
    "        scipy.stats as sp\n",
    "    '''\n",
    "    # limit dataframe to only the site\n",
    "    df = df[[site]]\n",
    "    \n",
    "    # filter dataframe to only include dates of interest\n",
    "    data = df[(df.index.to_datetime() > pd.datetime(begyear,1,1))&(df.index.to_datetime() < pd.datetime(endyear,1,1))]\n",
    "\n",
    "    # remove na values from dataframe\n",
    "    data = data.dropna()\n",
    "\n",
    "    # take average of each day of year (from 1 to 366) over the selected period of record\n",
    "    data['doy']=data.index.dayofyear\n",
    "    dailyavg = data[site].groupby(data['doy']).mean()\n",
    "        \n",
    "    data = np.sort(dailyavg)\n",
    "\n",
    "    ## uncomment the following to use normalized discharge instead of discharge\n",
    "    #mean = np.mean(data)\n",
    "    #std = np.std(data)\n",
    "    #data = [(data[i]-np.mean(data))/np.std(data) for i in range(len(data))]\n",
    "    data = [(data[i])/normalizer for i in range(len(data))]\n",
    "    \n",
    "    # ranks data from smallest to largest\n",
    "    ranks = sp.rankdata(data, method='average')\n",
    "\n",
    "    # reverses rank order\n",
    "    ranks = ranks[::-1]\n",
    "    \n",
    "    # calculate probability of each rank\n",
    "    prob = [(ranks[i]/(len(data)+1)) for i in range(len(data)) ]\n",
    "    \n",
    "    # plot data via matplotlib\n",
    "    plt.plot(prob,data,label=site+' '+str(begyear)+'-'+str(endyear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fdcmatch` uses Python's optimization capabilities to fit natural logarithim and exponential functions the flow duration curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fdcmatch(df, site, begyear=1900, endyear=2015, normalizer=1, fun=1):\n",
    "    '''\n",
    "    * This function creates a flow duration curve (or its inverse) and then matches a natural logrithmic function (or its inverse - exp) \n",
    "    to the flow duration curve\n",
    "    * The flow duration curve will be framed for averaged daily data for the duration of one year (366 days)\n",
    "    \n",
    "    PARAMETERS:\n",
    "        df = pandas dataframe of interest; must have a date or date-time as the index\n",
    "        site = pandas column containing discharge data; must be within df\n",
    "        begyear = beginning year of analysis; defaults to 1900\n",
    "        endyear = end year of analysis; defaults to 2015\n",
    "        normalizer = value to use to normalize discharge; defaults to 1 (no normalization)\n",
    "        fun = 1 for probability as a function of discharge; 0 for discharge as a function of probability; default=1 \n",
    "            * 1 will choose:\n",
    "                prob = a*ln(discharge*b+c)+d\n",
    "            * 0 will choose:\n",
    "                discharge = a*exp(prob*b+c)+d\n",
    "    RETURNS:\n",
    "        para, parb, parc, pard, r_squared_value, stderr\n",
    "    \n",
    "        par = modifying variables for functions = a,b,c,d\n",
    "        r_squared_value = r squared value for model\n",
    "        stderr = standard error of the estimate\n",
    "    \n",
    "    REQUIREMENTS:\n",
    "        pandas, scipy, numpy\n",
    "    '''\n",
    "    df = df[[site]]\n",
    "    \n",
    "    # filter dataframe to only include dates of interest\n",
    "    data = df[(df.index.to_datetime() > pd.datetime(begyear,1,1))&(df.index.to_datetime() < pd.datetime(endyear,1,1))]\n",
    "\n",
    "    # remove na values from dataframe\n",
    "    data = data.dropna()\n",
    "\n",
    "    # take average of each day of year (from 1 to 366) over the selected period of record\n",
    "    data['doy']=data.index.dayofyear\n",
    "    dailyavg = data[site].groupby(data['doy']).mean()\n",
    "        \n",
    "    data = np.sort(dailyavg)\n",
    "\n",
    "    ## uncomment the following to use normalized discharge instead of discharge\n",
    "    #mean = np.mean(data)\n",
    "    #std = np.std(data)\n",
    "    #data = [(data[i]-np.mean(data))/np.std(data) for i in range(len(data))]\n",
    "    data = [(data[i])/normalizer for i in range(len(data))]\n",
    "    \n",
    "    # ranks data from smallest to largest\n",
    "    ranks = sp.rankdata(data, method='average')\n",
    "\n",
    "    # reverses rank order\n",
    "    ranks = ranks[::-1]\n",
    "    \n",
    "    # calculate probability of each rank\n",
    "    prob = [(ranks[i]/(len(data)+1)) for i in range(len(data)) ]\n",
    " \n",
    "    # choose which function to use\n",
    "    try:\n",
    "        if fun==1:\n",
    "            # function to determine probability as a function of discharge\n",
    "            def func(x,a,b,c,d):\n",
    "                return a*np.log(x*b+c)+d\n",
    "\n",
    "            # matches func to data\n",
    "            par, cov = op.curve_fit(func, data, prob)\n",
    "\n",
    "            # checks fit of curve match\n",
    "            slope, interecept, r_value, p_value, stderr = \\\n",
    "            sp.linregress(prob, [par[0]*np.log(data[i]*par[1]+par[2])+par[3] for i in range(len(data))])\n",
    "        else:\n",
    "            # function to determine discharge as a function of probability\n",
    "            def func(x,a,b,c,d):\n",
    "                return a*np.exp(x*b+c)+d\n",
    "\n",
    "            # matches func to data\n",
    "            par, cov = op.curve_fit(func, prob, data)\n",
    "\n",
    "            # checks fit of curve match\n",
    "            slope, interecept, r_value, p_value, stderr = \\\n",
    "            sp.linregress(data,[par[0]*np.exp(prob[i]*par[1]+par[2])+par[3] for i in range(len(prob))])\n",
    "\n",
    "        # return parameters (a,b,c,d), r-squared of model fit, and standard error of model fit \n",
    "        return par[0], par[1], par[2], par[3], round(r_value**2,2), round(stderr,5)\n",
    "    except (RuntimeError,TypeError):\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list `sites` designates the USGS surface sites you are interested in analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sites = ['09309600','09309800',\n",
    "         '09310000','09310500','09310700','09311500','09312700','09313000',\n",
    "         '09317000','09317919','09317920','09317997','09318000','09318500','09319000',\n",
    "         '09323000','09324000','09324200','09324500','09325000','09325100','09326500','09327500','09327550',\n",
    "         '09330500','09331900','09331950','09332100',\n",
    "         '10148500',\n",
    "         '10205030','10206000','10206001','10208500',\n",
    "         '10210000','10211000','10215700','10215900','10216210','10216400','10217000']\n",
    "sitelab = ['Q'+site for site in sites]\n",
    "print(len(sites))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Stream Gage Discharge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run through all of the sites and import thier data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#d = {site: importusgssite(site) for site in sites}\n",
    "d = {}\n",
    "for site in sites:\n",
    "    #print site #use this line to error check site numbers\n",
    "    d[site] = importusgssite(site);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets merge all of the site data together in one dataframe, so that all of the daily data are aligned and together in one place.  We will call that one place `USGS_Site_Data`! While we are at it, we will add month, year, and day of year columns to make summarizing the data easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = {}\n",
    "f[sites[0]] = d[sites[0]]\n",
    "for i in range(len(sites)-1):\n",
    "    f[sites[i+1]] = pd.merge(d[sites[i+1]], f[sites[i]], left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USGS_Site_Data = f[sites[-1]]\n",
    "USGS_Site_Data['mon']=USGS_Site_Data.index.month\n",
    "USGS_Site_Data['yr']=USGS_Site_Data.index.year\n",
    "USGS_Site_Data['dy']=USGS_Site_Data.index.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USGS_Site_Data.to_csv(dataroot+'USGS_Site_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Stream Gage Site Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should import the information on each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = {}\n",
    "for site in sites:\n",
    "    z[site] = getusgssiteinfo(site);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did with the data, we will combine all of the site information together in one table, so that it is easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USGS_Site_Info = pd.concat(z)\n",
    "USGS_Site_Info = USGS_Site_Info.reset_index()\n",
    "USGS_Site_Info = USGS_Site_Info.drop(['level_1'],axis=1)\n",
    "USGS_Site_Info = USGS_Site_Info.set_index(['level_0'])\n",
    "USGS_Site_Info = USGS_Site_Info.drop(['code'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets extract the measurement start and end dates from the station data, as well as some basic summary statistics.  We can tack this information onto the site information table we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = {}\n",
    "m = {}\n",
    "for site in sites:\n",
    "    q[site] = USGS_Site_Data[site].first_valid_index()\n",
    "    m[site] = USGS_Site_Data[site].last_valid_index()\n",
    "\n",
    "USGS_start_date = pd.DataFrame(data=q, index=[0])\n",
    "USGS_finish_date = pd.DataFrame(data=m, index=[0])\n",
    "USGS_start_date = USGS_start_date.transpose()\n",
    "USGS_start_date['start_date'] = USGS_start_date[0]\n",
    "USGS_start_date = USGS_start_date.drop([0],axis=1)\n",
    "USGS_finish_date = USGS_finish_date.transpose()\n",
    "USGS_finish_date['fin_date'] = USGS_finish_date[0]\n",
    "USGS_finish_date = USGS_finish_date.drop([0],axis=1)\n",
    "USGS_start_fin = pd.merge(USGS_finish_date,USGS_start_date, left_index=True, right_index=True, how='outer' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USGS_Site_Info = pd.merge(USGS_start_fin,USGS_Site_Info, left_index=True, right_index=True, how='outer' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USGS_sum_stats = USGS_Site_Data[sites].describe()\n",
    "USGS_sum_stats = USGS_sum_stats.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USGS_Site_Info = pd.merge(USGS_sum_stats,USGS_Site_Info, left_index=True, right_index=True, how='outer' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line allows us to save the site information table to a clipboard, which can be pasted into a spreadsheet, for those who like visualizing information in Excel and similar products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USGS_Site_Info.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the site information to generate a <a href=https://en.wikipedia.org/wiki/Gantt_chart>Gantt chart</a>, showing the duration that each station measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# designate variables\n",
    "x2 = USGS_Site_Info['fin_date'].astype(datetime).values\n",
    "x1 = USGS_Site_Info['start_date'].astype(datetime).values\n",
    "y = USGS_Site_Info.index.astype(np.int)\n",
    "names = USGS_Site_Info['name'].values\n",
    "\n",
    "labs, tickloc, col = [], [], []\n",
    "\n",
    "# create color iterator for multi-color lines in gantt chart\n",
    "color=iter(cm.Dark2(np.linspace(0,1,len(y))))\n",
    "\n",
    "plt.figure(figsize=[8,10])\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# generate a line and line properties for each station\n",
    "for i in range(len(y)):\n",
    "    c=next(color)\n",
    "    \n",
    "    plt.hlines(i+1, x1[i], x2[i], label=y[i], color=c, linewidth=2)\n",
    "    labs.append(names[i].title()+\" (\"+str(y[i])+\")\")\n",
    "    tickloc.append(i+1)\n",
    "    col.append(c)\n",
    "plt.ylim(0,len(y)+1)\n",
    "plt.yticks(tickloc, labs)\n",
    "\n",
    "# create custom x labels\n",
    "plt.xticks(np.arange(datetime(np.min(x1).year,1,1),np.max(x2)+timedelta(days=365.25),timedelta(days=365.25*5)),rotation=45)\n",
    "plt.xlim(datetime(np.min(x1).year,1,1),np.max(x2)+timedelta(days=365.25))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('USGS Official Station Name and Station Id')\n",
    "plt.grid()\n",
    "plt.title('USGS Station Measurement Duration')\n",
    "# color y labels to match lines\n",
    "gytl = plt.gca().get_yticklabels()\n",
    "for i in range(len(gytl)):\n",
    "    gytl[i].set_color(col[i])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(rootname+'gantt.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script will generate a series of box and whisker plots and save them in a pdf. It makes a box plot for each station, breaking the data into monthly intervals.  Make sure to change the directory name in the script so it ends up in a recognizable place on your computer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dictionary of integers and their month equivalent\n",
    "months = {'1':'Jan.', '2':'Feb.', '3':'Mar.', '4':'Apr.', '5':'May', '6':'Jun.', \n",
    "         '7':'Jul.', '8':'Aug.', '9':'Sep.', '10':'Oct.', '11':'Nov.', '12':'Dec.', 'Total':'Total'}\n",
    "# create empty dictionary to hold pandas Dataframes\n",
    "j = {}\n",
    "\n",
    "\n",
    "with PdfPages(rootname + 'station_boxplots.pdf') as pdfs:\n",
    "    ymax = 10000\n",
    "    ymin = 0.01\n",
    "    for i in range(len(sites)):\n",
    "        # make a dataframe containing summary statistics and store it in the j dictionary\n",
    "        j[sites[i]] = USGS_Site_Data.groupby('mon')[sites[i]].agg({'min':np.min, 'mean':np.mean, \n",
    "                                                                   'median':np.median, 'max':np.max, 'std':np.std, \n",
    "                                                                   'cnt':(lambda x: np.count_nonzero(~np.isnan(x)))}).reset_index()\n",
    "        # make a list of the custom lables you will use for your boxplot; this one will show the number of samples used to make the plot\n",
    "        labs = [months[(str(j[sites[i]]['mon'][b]))] + \" (n=\" + str(int(j[sites[i]]['cnt'][b])) + \")\" for b in range(len(j[sites[i]]))]\n",
    "        # designate the location of each custom label\n",
    "        tickloc = [b+1 for b in range(len(j[sites[i]]['mon']))]\n",
    "        \n",
    "        plt.figure()\n",
    "        USGS_Site_Data.boxplot(column=sites[i],by='mon', rot=70)\n",
    "        strtdt = str(USGS_Site_Info.ix[sites[i],'start_date'])[0:10]\n",
    "        findt = str(USGS_Site_Info.ix[sites[i],'fin_date'])[0:10]\n",
    "        siteName = USGS_Site_Info.ix[sites[i],'name'].title() \n",
    "        plt.title( siteName + ' (' + sites[i] + ')  ' + strtdt + ' to ' + findt )\n",
    "        plt.suptitle('')\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Discharge (cfs)')\n",
    "        plt.ylim((ymin,ymax))\n",
    "        plt.xlabel('Month')\n",
    "        # here is where your lists for the custom label come into play\n",
    "        plt.xticks(tickloc, labs)\n",
    "        \n",
    "        pdfs.savefig()\n",
    "\n",
    "        plt.close()\n",
    "    # Save metadata of the pdf so you can find it later\n",
    "    d = pdfs.infodict()\n",
    "    d['Title'] = 'Monthly Station USGS Boxplots UMSS'\n",
    "    d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "    d['Subject'] = 'Boxplots of several USGS Surface Stations'\n",
    "    d['Keywords'] = 'USGS Surface NWIS Boxplot'\n",
    "    d['CreationDate'] = datetime.today()\n",
    "    d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few of the boxplots so you can see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,3):\n",
    "    j[sites[i]] = USGS_Site_Data.groupby('mon')[sites[i]].agg([np.min, np.mean, np.median, np.max, np.std, np.size]).reset_index()\n",
    "    # make a list of the custom lables you will use for your boxplot; this one will show the number of samples used to make the plot\n",
    "    labs = [months[(str(j[sites[i]]['mon'][b]))] + \" (n=\" + str(int(j[sites[i]]['size'][b])) + \")\" for b in range(len(j[sites[i]]))]\n",
    "    # designate the location of each custom label\n",
    "    tickloc = [b+1 for b in range(len(j[sites[i]]['mon']))]\n",
    "\n",
    "    plt.figure()\n",
    "    USGS_Site_Data.boxplot(column=sites[i],by='mon', rot=70)\n",
    "    plt.title(USGS_Site_Info.ix[sites[i],'name'].title() + ' (' + sites[i] + ')')\n",
    "    plt.suptitle('')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('Discharge (cfs)')\n",
    "    plt.ylim((ymin,ymax))\n",
    "    plt.xlabel('Month')\n",
    "    # here is where your lists for the custom label come into play\n",
    "    plt.xticks(tickloc, labs)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will generate boxplots showing all of the station data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This script summarizes discharge for all sites and limits the number of box plots on one graph to the n variable\n",
    "j=0\n",
    "with PdfPages(rootname + 'sum_boxplots.pdf') as pdf:\n",
    "    while j < len(sites):\n",
    "        ymax = 10000\n",
    "        ymin = 0.01\n",
    "        n=10\n",
    "        # if statement allows for uneven number of sites on last page\n",
    "        if j+n >= len(sites):\n",
    "            plt.figure()\n",
    "            USGS_Site_Data[sites[j:-1]].plot(kind='box')\n",
    "            plt.title('Sites '+sites[j]+' to '+sites[-1] )\n",
    "            plt.yscale('log')\n",
    "            plt.xlabel('USGS Site')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel('discharge (cfs)')\n",
    "            plt.ylim((ymin,ymax))\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            j = j+n\n",
    "        else:\n",
    "            plt.figure()\n",
    "            USGS_Site_Data[sites[j:j+n]].plot(kind='box')\n",
    "            plt.title('Sites '+sites[j]+' to '+sites[j+n] )\n",
    "            plt.yscale('log')\n",
    "            plt.xlabel('USGS Site')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel('discharge (cfs)')\n",
    "            plt.ylim((ymin,ymax))\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            j = j+n\n",
    "        # Save metadata of the pdf so you can find it later\n",
    "        d = pdf.infodict()\n",
    "        d['Title'] = 'Summary USGS Boxplots UMSS'\n",
    "        d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "        d['Subject'] = 'Boxplots of several USGS Surface Stations'\n",
    "        d['Keywords'] = 'USGS Surface NWIS Boxplot'\n",
    "        d['CreationDate'] = datetime.today()\n",
    "        d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also produce hydrographs of each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xmax = USGS_Site_Data.index.astype(datetime).values[-1]\n",
    "xmin = USGS_Site_Data.index.astype(datetime).values[0]\n",
    "\n",
    "pdfs = PdfPages(rootname + 'station_hydrographs.pdf')\n",
    "ymax = 10000\n",
    "ymin = 0.1\n",
    "for i in range(len(sites)):\n",
    "    x = USGS_Site_Data.index.values\n",
    "    y = USGS_Site_Data[sites[i]].values\n",
    "    plt.figure()\n",
    "    plt.plot(x,y)\n",
    "    strtdt = str(USGS_Site_Info.ix[sites[i],'start_date'])[0:10]\n",
    "    findt = str(USGS_Site_Info.ix[sites[i],'fin_date'])[0:10]\n",
    "    siteName = USGS_Site_Info.ix[sites[i],'name'].title() \n",
    "    plt.title( siteName + ' (' + sites[i] + ')  ' + strtdt + ' to ' + findt )\n",
    "    plt.suptitle('')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('Discharge (cfs)')\n",
    "    plt.ylim((ymin,ymax))\n",
    "    plt.xlabel('Year')\n",
    "    plt.xticks(np.arange(datetime(1905,1,1),xmax+timedelta(days=365.25),timedelta(days=365.25*5)),rotation=45)\n",
    "    plt.xlim(xmin,xmax)\n",
    "    pdfs.savefig()\n",
    "    plt.close()\n",
    "    # Save metadata of the pdf so you can find it later\n",
    "\n",
    "d = pdfs.infodict()\n",
    "d['Title'] = 'Monthly Station USGS Hydrographs UMSS'\n",
    "d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "d['Subject'] = 'Hydrograph of several USGS Surface Stations'\n",
    "d['Keywords'] = 'USGS Surface NWIS Hydrograph'\n",
    "d['CreationDate'] = datetime.today()\n",
    "d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.date_range(start=xmin, end=xmax, freq='5AS').year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmax = USGS_Site_Data.index[-1]\n",
    "xmin = USGS_Site_Data.index[0]\n",
    "\n",
    "plt.figure()\n",
    "ticks = pd.date_range(start=xmin, end=xmax, freq='4AS')\n",
    "USGS_Site_Data[sites[0:3]].plot(subplots=True,sharex=True,figsize=(10,8),logy=True, rot=90)\n",
    "plt.xlim(xmin,xmax)\n",
    "labs = pd.date_range(start=xmin, end=xmax, freq='4AS').year\n",
    "plt.xticks(ticks,labs)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lumped_hydro(i1,i2):\n",
    "    pdfs = PdfPages(rootname + 'station_hydrographs_lumped.pdf')\n",
    "    plt.figure()\n",
    "    ticks = pd.date_range(start=xmin, end=xmax, freq='4AS')\n",
    "    USGS_Site_Data[sites[i1:i2]].plot(subplots=True, sharex=True, figsize=(10,24),logy=True, rot=90)\n",
    "    plt.xlim(xmin,xmax)\n",
    "    labs = pd.date_range(start=xmin, end=xmax, freq='4AS').year\n",
    "    plt.xticks(ticks,labs)\n",
    "    pdfs.savefig()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lumped_hydro(0,10)\n",
    "lumped_hydro(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lumped_hydro(20,30)\n",
    "lumped_hydro(30,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will iteratively produce Flow Duration Curves for each of the stations. A flow duration curve is a <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#PPF>percent point function (ppf)</a>, displaying discharge as a function of probability of that discharge occuring. The ppf is the inverse of the better known <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#CDF>cumulative distribution function (cdf)</a>. See <a href=http://pubs.usgs.gov/wsp/1542a/report.pdf>this USGS publication</a> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages(rootname+'station_fdc.pdf') as pdf:\n",
    "    ymax = 10000\n",
    "    ymin = 0.01\n",
    "    for i in range(len(sites)):\n",
    "        plt.figure()\n",
    "        fdc_simple(USGS_Site_Data,sites[i],1900,2015)\n",
    "        fdc_simple(USGS_Site_Data,sites[i],1900,1970)\n",
    "        fdc_simple(USGS_Site_Data,sites[i],1970,2015)\n",
    "        plt.ylim(0.01,10000)\n",
    "        plt.xlim(-.05,1.05)\n",
    "        plt.grid(which = 'both')\n",
    "        plt.legend()\n",
    "        plt.xlabel('probability that discharge was exceeded or equaled')\n",
    "        plt.title('Flow duration curve for ' + str(USGS_Site_Info['name'][i]).title() + ' ('+ sites[i] +')'+'\\n'+\n",
    "                  'Record: ' + str(USGS_Site_Info['start_date'][i])[0:10] + ' to ' + str(USGS_Site_Info['fin_date'][i])[0:10])\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('discharge (cfs)')\n",
    "        plt.xticks(np.arange(0,1.05,0.05))\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "    # Save metadata of the pdf so you can find it later\n",
    "    d = pdf.infodict()\n",
    "    d['Title'] = 'Flow Duration Curves USGS'\n",
    "    d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "    d['Subject'] = 'Flow Duration Curves of several USGS Surface Stations'\n",
    "    d['Keywords'] = 'USGS Surface NWIS FDC Flow Duration'\n",
    "    d['CreationDate'] = datetime.today()\n",
    "    d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For brevity, here is an example plot from the output of the script above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fdc_simple(USGS_Site_Data,sites[38],1900,2015)\n",
    "fdc_simple(USGS_Site_Data,sites[38],1900,1970)\n",
    "fdc_simple(USGS_Site_Data,sites[38],1970,2015)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid(which = 'both')\n",
    "plt.xlabel('% of time that indicated discharge was exceeded or equaled')\n",
    "plt.ylabel('discharge (cfs)')\n",
    "plt.xticks(np.arange(0.0,1.05,0.05))\n",
    "plt.title('Flow duration curve for ' + str(USGS_Site_Info['name'][38]) + ' ('+ sites[i] +')'+'\\n'+ \n",
    "          'Record: ' + str(USGS_Site_Info['start_date'][i])[0:10] + ' to ' + str(USGS_Site_Info['fin_date'][i])[0:10])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some sad attempts to model the flow duration curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site = sites[28]\n",
    "df = USGS_Site_Data[[site]]\n",
    "begyear = 1900\n",
    "endyear = 2015\n",
    "data = df[(df.index.to_datetime() > pd.datetime(begyear,1,1))&(df.index.to_datetime() < pd.datetime(endyear,1,1))]\n",
    "data = data.dropna()\n",
    "\n",
    "data['doy']=data.index.dayofyear\n",
    "dailyavg = data[site].groupby(data['doy']).mean()\n",
    "\n",
    "\n",
    "data = np.sort(dailyavg)\n",
    "\n",
    "mean = np.mean(data)\n",
    "std = np.std(data)\n",
    "f = [(data[i]) for i in range(len(data))]\n",
    "#f = [(data[i])/mean for i in range(len(data))]\n",
    "#f = [(data[i]-std)/mean for i in range(len(data))]\n",
    "\n",
    "ranks = sp.rankdata(f, method='average')\n",
    "ranks = ranks[::-1]\n",
    "prob = [(ranks[i]/(len(f)+1)) for i in range(len(f)) ]\n",
    "\n",
    "x = prob\n",
    "y = f\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y,x,label=site,color='blue')\n",
    "#plt.yscale('log')\n",
    "#plt.xlim(-.05,1.05)\n",
    "plt.grid(which = 'both')\n",
    "plt.ylabel('probability that discharge was exceeded or equaled')\n",
    "plt.xlabel('normalized discharge')\n",
    "#plt.xticks(np.arange(0,1.00,0.05))\n",
    "plt.title('Flow duration curve for ' + site + ' averaged from ' + str(begyear) + ' to ' + str(endyear))\n",
    "\n",
    "def func(x,a,b,c,d):\n",
    "    return a*np.log(x*b+c)+d\n",
    "\n",
    "par, cov = op.curve_fit(func,y,x,p0=[-0.16528617, 1.54535185, -24.70440088, 0.9])\n",
    "plt.plot(y, [par[0]*np.log(y[i]*par[1]+par[2])+par[3] for i in range(len(y))], color='red')\n",
    "print 'curve fit', sp.linregress(x,[par[0]*np.log(y[i]*par[1]+par[2])+par[3] for i in range(len(y))])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x,y,label=site,color='blue')\n",
    "#plt.yscale('log')\n",
    "#plt.xlim(-.05,1.05)\n",
    "plt.grid(which = 'both')\n",
    "plt.xlabel('probability that discharge was exceeded or equaled')\n",
    "plt.ylabel('normalized discharge')\n",
    "#plt.xticks(np.arange(0,1.00,0.05))\n",
    "plt.title('Flow duration curve for ' + site + ' averaged from ' + str(begyear) + ' to ' + str(endyear))\n",
    "\n",
    "def func2(x,a,b,c,d):\n",
    "    return a*np.exp(x*b+c)+d\n",
    "\n",
    "\n",
    "parm, covm = op.curve_fit(func2,x,y,p0=[-0.16528617, 0.02, 0.70440088, 0.9])\n",
    "plt.plot(x, [parm[0]*np.exp(x[i]*parm[1]+parm[2])+parm[3] for i in range(len(x))], color='red')\n",
    "print 'curve fit', sp.linregress(y,[parm[0]*np.exp(x[i]*parm[1]+parm[2])+parm[3] for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dic2df(dic,head):\n",
    "    df = pd.DataFrame(data=dic)\n",
    "    df = df.transpose()\n",
    "    df.columns = [str(head)+'_var1',str(head)+'_var2',str(head)+'_var3',str(head)+'_var4',str(head)+'_r2',str(head)+'_err']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = {}; m = {}; n = {}; k = {}; j = {}; p = {}\n",
    "\n",
    "for site in sites:\n",
    "    q[site] = fdcmatch(USGS_Site_Data,site,1900,2015,1,1)\n",
    "    m[site] = fdcmatch(USGS_Site_Data,site,1900,1970,1,1)\n",
    "    n[site] = fdcmatch(USGS_Site_Data,site,1970,2015,1,1)\n",
    "    k[site] = fdcmatch(USGS_Site_Data,site,1900,2015,1,0)\n",
    "    j[site] = fdcmatch(USGS_Site_Data,site,1900,1970,1,0)\n",
    "    p[site] = fdcmatch(USGS_Site_Data,site,1970,2015,1,0)\n",
    "    \n",
    "dics = [q,m,n,k,j,p]\n",
    "heads = ['all','to70','fm70','allin','to70in','fm70in']\n",
    "\n",
    "USGS_q = dic2df(q,'all')\n",
    "USGS_m = dic2df(m,'to70')\n",
    "USGS_parms = pd.merge(USGS_q,USGS_m, left_index=True, right_index=True, how='outer' )\n",
    "\n",
    "for i in range(2,6,1):\n",
    "    x = dic2df(dics[i],heads[i])\n",
    "    USGS_parms = pd.merge(USGS_parms,x, left_index=True, right_index=True, how='outer' )\n",
    "\n",
    "USGS_parms.to_clipboard()\n",
    "#USGS_finish_date = USGS_finish_date.drop([0],axis=1)\n",
    "#USGS_start_fin = pd.merge(USGS_finish_date,USGS_start_date, left_index=True, right_index=True, how='outer' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations from the modeled fdc plots can be used to estimate the discharge for a site based on data from a similar site.  However, the results are mediocre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n1 = 12\n",
    "n2 = 11\n",
    "\n",
    "USGS_Site_Data[sites[n1]+'p'] = [USGS_parms['all_var1'][n1]*np.log(USGS_Site_Data[sites[n1]][i]*USGS_parms['all_var2'][n1]+\\\n",
    "                                                                   USGS_parms['all_var3'][n1])+USGS_parms['all_var4'][n1] for i in \\\n",
    "range(len(USGS_Site_Data[sites[n1]]))]\n",
    "\n",
    "USGS_Data = USGS_Site_Data[USGS_Site_Data[sites[n1]+'p']>0]\n",
    "\n",
    "USGS_Data[sites[n2]+'d'] = [USGS_parms['allin_var1'][n2]*np.exp(USGS_Data[sites[n1]+'p'][i]*USGS_parms['allin_var2'][n2]+\\\n",
    "                                                                   USGS_parms['allin_var3'][n2])+USGS_parms['allin_var4'][n2] for i in \\\n",
    "range(len(USGS_Data[sites[n1]+'p']))]\n",
    "\n",
    "y1 = USGS_Data[sites[n2]+'d'].values\n",
    "y2 = USGS_Site_Data[sites[n2]].values\n",
    "\n",
    "x1 = USGS_Data.index.to_datetime()\n",
    "x2 = USGS_Site_Data.index.to_datetime()\n",
    "y3 = USGS_Data[sites[n1]]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x1,y1, label=\"modeled discharge\")\n",
    "plt.plot(x2,y2, label=\"actual discharge\")\n",
    "plt.plot(x1,y3, label=\"reference discharge\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "#plt.xlim(USGS_Site_Info['start_date'][n2],USGS_Site_Info['fin_date'][n2])\n",
    "plt.xlim('1/1/1970','1/1/1974')\n",
    "#plt.ylim(1,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stackoverflow.com/questions/15408371/cumulative-distribution-plots-python <br>\n",
    "http://hydroclimpy.sourceforge.net/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following script if you want to see a map of your stations.  This assumes that you have the <a href=http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/>Basemap package</a> installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "X = USGS_Site_Info['longitude'].astype(float).values.tolist()\n",
    "Y = USGS_Site_Info['latitude'].astype(float).values.tolist()\n",
    "\n",
    "n = 0.05 \n",
    "m = Basemap(llcrnrlon=min(X)+n,llcrnrlat=min(Y)+n,urcrnrlon=max(X)+n,urcrnrlat=max(Y)+n,\n",
    "            resolution='h',projection='cyl',lon_0=np.mean(X),lat_0=np.mean(Y))\n",
    "m.drawrivers(color='blue',linewidth=0.5)\n",
    "m.drawcounties(color='red',linewidth=0.5)\n",
    "m.arcgisimage()\n",
    "#m.etopo(scale=0.5)\n",
    "lons = X\n",
    "lats = Y\n",
    "x,y = m(lons,lats)\n",
    "m.plot(x,y,'ro', markersize=8)\n",
    "\n",
    "#m.drawmapscale(lon=-114, lat=43.5, length=100, lon0=-114, lat0=39, barstyle='simple', units='km')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col = USGS_Site_Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects as robj\n",
    "import rpy2.rlike.container as rlc\n",
    "\n",
    "def pandas_data_frame_to_rpy2_data_frame(pDataframe):\n",
    "    orderedDict = rlc.OrdDict()\n",
    "\n",
    "    for columnName in pDataframe:\n",
    "        columnValues = pDataframe[columnName].values\n",
    "        filteredValues = [value if pd.notnull(value) else robj.NA_Real \n",
    "                          for value in columnValues]\n",
    "\n",
    "        try:\n",
    "            orderedDict[columnName] = robj.FloatVector(filteredValues)\n",
    "        except ValueError:\n",
    "            orderedDict[columnName] = robj.StrVector(filteredValues)\n",
    "\n",
    "    rDataFrame = robj.DataFrame(orderedDict)\n",
    "    rDataFrame.rownames = robj.StrVector(pDataframe.index)\n",
    "\n",
    "    return rDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USD = pandas_data_frame_to_rpy2_data_frame(USGS_Site_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
