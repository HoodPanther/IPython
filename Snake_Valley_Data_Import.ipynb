{
 "metadata": {
  "name": "",
  "signature": "sha256:2c2f1a83664469027cf3fbdb937c5e9bc915a949b9aec00d7ba21588d6641896"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import os\n",
      "import glob\n",
      "import xmltodict\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.dates as dates\n",
      "import matplotlib.ticker as tick\n",
      "import seaborn as sns\n",
      "import scipy.stats as sp\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.tsa.tsatools as tools\n",
      "from pandas.stats.api import ols\n",
      "from datetime import datetime\n",
      "from pylab import rcParams\n",
      "rcParams['figure.figsize'] = 15, 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Drive = 'D'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getfilename(path):\n",
      "    # this function extracts the file name without file path or extension\n",
      "    return path.split('\\\\').pop().split('/').pop().rsplit('.', 1)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Compilation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The script below uses a complete file path with wildcard ending to find all of the relevant files in a directory (folder) to generate a Pandas dataframe of all of the compiled data, with duplicates removed and datetimes sorted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compilation(inputfile):\n",
      "    \"\"\"\n",
      "    This function reads multiple Solinst transducer files in a directory and generates a compiled Pandas dataframe.\n",
      "    \n",
      "    inputfile = complete file path to input files; use * for wildcard in file name\n",
      "        example -> 'O:\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\all\\\\LEV\\\\*baro*' picks any file containing 'baro'\n",
      "    \n",
      "    packages required:\n",
      "        pandas as pd\n",
      "        glob\n",
      "        os\n",
      "        xmltodict\n",
      "    \"\"\"\n",
      "        \n",
      "    # create empty dictionary to hold dataframes\n",
      "    f={}\n",
      "\n",
      "    # generate list of relevant files\n",
      "    filelist = glob.glob(inputfile)\n",
      "\n",
      "    # iterate through list of relevant files\n",
      "    for infile in filelist:\n",
      "        # get the extension of the input file\n",
      "        filetype = os.path.splitext(infile)[1]\n",
      "        # run computations using lev files\n",
      "        if filetype=='.lev':\n",
      "            # open text file\n",
      "            with open(infile) as fd:\n",
      "                # find beginning of data\n",
      "                indices = fd.readlines().index('[Data]\\n')\n",
      "\n",
      "            # convert data to pandas dataframe starting at the indexed data line\n",
      "            f[getfilename(infile)] = pd.read_table(infile, parse_dates=True, sep='     ', index_col=0,\n",
      "                                           skiprows=indices+2, names=['DateTime','Level','Temperature'], skipfooter=1,engine='python')\n",
      "            # add extension-free file name to dataframe\n",
      "            f[getfilename(infile)]['name'] = getfilename(infile)\n",
      "            \n",
      "        # run computations using xle files\n",
      "        elif filetype=='.xle':\n",
      "            # open text file\n",
      "            with open(infile) as fd:\n",
      "                # parse xml\n",
      "                obj = xmltodict.parse(fd.read(),encoding=\"ISO-8859-1\")\n",
      "            # navigate through xml to the data\n",
      "            wellrawdata = obj['Body_xle']['Data']['Log']\n",
      "            # convert xml data to pandas dataframe\n",
      "            f[getfilename(infile)] = pd.DataFrame(wellrawdata)\n",
      "            # get header names and apply to the pandas dataframe          \n",
      "            f[getfilename(infile)][str(obj['Body_xle']['Ch1_data_header']['Identification']).title()] = f[getfilename(infile)]['ch1']\n",
      "            f[getfilename(infile)][str(obj['Body_xle']['Ch2_data_header']['Identification']).title()] = f[getfilename(infile)]['ch2']\n",
      "  \n",
      "            # add extension-free file name to dataframe\n",
      "            f[getfilename(infile)]['name'] = getfilename(infile)\n",
      "            # combine Date and Time fields into one field\n",
      "            f[getfilename(infile)]['DateTime'] = pd.to_datetime(f[getfilename(infile)].apply(lambda x: x['Date'] + ' ' + x['Time'], 1))\n",
      "            f[getfilename(infile)] = f[getfilename(infile)].reset_index()\n",
      "            f[getfilename(infile)] = f[getfilename(infile)].set_index('DateTime')\n",
      "            f[getfilename(infile)] = f[getfilename(infile)].drop(['Date','Time','@id','ch1','ch2','index','ms'],axis=1)\n",
      "        # run computations using csv files\n",
      "\n",
      "        else:\n",
      "            pass\n",
      "    # concatonate all of the dataframes in dictionary f to one dataframe: g\n",
      "    g = pd.concat(f)\n",
      "    # remove multiindex and replace with index=Datetime\n",
      "    g = g.reset_index()\n",
      "    g = g.set_index(['DateTime'])\n",
      "    # drop old indexes\n",
      "    g = g.drop(['level_0'],axis=1)\n",
      "    # remove duplicates based on index then sort by index\n",
      "    g['ind']=g.index\n",
      "    g.drop_duplicates(subset='ind',inplace=True)\n",
      "    g.drop('ind',axis=1,inplace=True)\n",
      "    g = g.sort()\n",
      "    # ensure that the Level and Temperature data are in a float format\n",
      "    g['Level'] = g['Level'].convert_objects(convert_numeric=True)\n",
      "    g['Temperature'] = g['Temperature'].convert_objects(convert_numeric=True)\n",
      "    outfile = g\n",
      "    return outfile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Appendomatic"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The appendomatic function below appends new data (infile) onto existing data by parsing the new data and making it the same format as the existing data, then merging the datasets, then overwriting the existing data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def appendomatic(infile,existingfile):\n",
      "    '''\n",
      "    appends data from one table to an existing compilation\n",
      "    this tool will delete and replace the existing file\n",
      "\n",
      "    infile = input file\n",
      "    existingfile = file you wish to append to\n",
      "    '''\n",
      "\n",
      "    # get the extension of the input file\n",
      "    filetype = os.path.splitext(infile)[1]\n",
      "    \n",
      "    # run computations using lev files\n",
      "    if filetype=='.lev':\n",
      "        # open text file\n",
      "        with open(infile) as fd:\n",
      "            # find beginning of data\n",
      "            indices = fd.readlines().index('[Data]\\n')\n",
      "\n",
      "        # convert data to pandas dataframe starting at the indexed data line\n",
      "        f = pd.read_table(infile, parse_dates=True, sep='     ', index_col=0,\n",
      "                                       skiprows=indices+2, names=['DateTime','Level','Temperature'], skipfooter=1,engine='python')\n",
      "        # add extension-free file name to dataframe\n",
      "        f['name'] = getfilename(infile)\n",
      "\n",
      "    # run computations using xle files\n",
      "    elif filetype=='.xle':\n",
      "        # open text file\n",
      "        with open(infile) as fd:\n",
      "            # parse xml\n",
      "            obj = xmltodict.parse(fd.read(),encoding=\"ISO-8859-1\")\n",
      "        # navigate through xml to the data\n",
      "        wellrawdata = obj['Body_xle']['Data']['Log']\n",
      "        # convert xml data to pandas dataframe\n",
      "        f = pd.DataFrame(wellrawdata)\n",
      "        # get header names and apply to the pandas dataframe\n",
      "        f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()] = f['ch1']\n",
      "        f[str(obj['Body_xle']['Ch2_data_header']['Identification']).title()] = f['ch2']\n",
      "\n",
      "        # add extension-free file name to dataframe\n",
      "        f['name'] = getfilename(infile)\n",
      "        # combine Date and Time fields into one field\n",
      "        f['DateTime'] = pd.to_datetime(f.apply(lambda x: x['Date'] + ' ' + x['Time'], 1))\n",
      "        f = f.reset_index()\n",
      "        f = f.set_index('DateTime')\n",
      "        f = f.drop(['Date','Time','@id','ch1','ch2','index','ms'],axis=1)\n",
      "    \n",
      "    # run computations using csv files\n",
      "    elif filetype=='.csv':\n",
      "        with open(infile) as fd:\n",
      "        # find beginning of data\n",
      "            try:\n",
      "                indices = fd.readlines().index('Date,Time,ms,Level,Temperature\\n')\n",
      "            except ValueError:\n",
      "                indices = fd.readlines().index(',Date,Time,100 ms,Level,Temperature\\n')\n",
      "        f = pd.read_csv(infile, skiprows=indices, skipfooter=1, engine='python')\n",
      "        # add extension-free file name to dataframe\n",
      "        f['name'] = getfilename(infile)\n",
      "        # combine Date and Time fields into one field\n",
      "        f['DateTime'] = pd.to_datetime(f.apply(lambda x: x['Date'] + ' ' + x['Time'], 1))\n",
      "        f = f.reset_index()\n",
      "        f = f.set_index('DateTime')\n",
      "        f = f.drop(['Date','Time','ms','index'],axis=1)\n",
      "            # skip other file types\n",
      "    else:\n",
      "        pass\n",
      "\n",
      "    # ensure that the Level and Temperature data are in a float format\n",
      "    f['Level'] = f['Level'].convert_objects(convert_numeric=True)\n",
      "    f['Temperature'] = f['Temperature'].convert_objects(convert_numeric=True)\n",
      "    h = pd.read_csv(existingfile,index_col=0,header=0,parse_dates=True)\n",
      "    g = pd.concat([h,f])\n",
      "    # remove duplicates based on index then sort by index\n",
      "    g['ind']=g.index\n",
      "    g.drop_duplicates(subset='ind',inplace=True)\n",
      "    g.drop('ind',axis=1,inplace=True)\n",
      "    g = g.sort()    \n",
      "    os.remove(existingfile)\n",
      "    g.to_csv(existingfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Make Files Table "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_files_table(folder, wellinfofile):\n",
      "    '''\n",
      "    This tool will make a descriptive table (Pandas DataFrame) containing filename, date, and site id.\n",
      "    For it to work properly, files must be named in the following fashion:\n",
      "    siteid YYYY-MM-DD\n",
      "    example: pw03a 2015-03-15.csv\n",
      "\n",
      "    This tool assumes that if there are two files with the same name but different extensions, \n",
      "    then the datalogger for those data is a Solinst datalogger.\n",
      "\n",
      "    folder = directory containing the newly downloaded transducer data\n",
      "    '''\n",
      "\n",
      "    filenames = next(os.walk(folder))[2]\n",
      "    site_id, exten, dates, fullfilename = [],[],[],[]\n",
      "    # parse filename into relevant pieces\n",
      "    for i in filenames:\n",
      "        site_id.append(i[:-15].lower())\n",
      "        exten.append(i[-4:])\n",
      "        dates.append(i[-14:-4])\n",
      "        fullfilename.append(i)\n",
      "    files = {'siteid':site_id,'extensions':exten,'date':dates,'full_file_name':fullfilename}\n",
      "    files = pd.DataFrame(files)\n",
      "    files['filedups'] = files.duplicated(subset='siteid')\n",
      "    files['LoggerTypeID'] = files['filedups'].astype('int')+1\n",
      "    files['LoggerTypeName']=files['LoggerTypeID'].apply(lambda x: 'Solinst' if x==2 else 'Global Water')\n",
      "    files.drop_duplicates(subset='siteid',take_last=True,inplace=True)\n",
      "\n",
      "    #wellinfo = pd.read_csv(wellinfofile,header=0,index_col=0)\n",
      "    wellinfo[\"G_Elev_m\"] = wellinfo[\"GroundElevation\"]/3.2808\n",
      "    wellinfo['Well'] = wellinfo['Well'].apply(lambda x: str(x).lower().strip())\n",
      "    files = pd.merge(files,wellinfo,left_on='siteid',right_on='Well')\n",
      "    \n",
      "    return files"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Barodistance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def barodistance(wellinfo):\n",
      "    '''\n",
      "    Determines Closest Barometer to Each Well using wellinfo DataFrame\n",
      "    '''\n",
      "    barometers = {'barom':['pw03','pw10','pw19'], 'X':[240327.49,271127.67,305088.9], \n",
      "                  'Y':[4314993.95,4356071.98,4389630.71], 'Z':[1623.079737,1605.187759,1412.673738]}\n",
      "    barolocal = pd.DataFrame(barometers)\n",
      "    barolocal = barolocal.reset_index()\n",
      "    barolocal.set_index('barom',inplace=True)\n",
      "\n",
      "    wellinfo['pw03'] = np.sqrt((barolocal.loc['pw03','X']-wellinfo['UTMEasting'])**2 + \n",
      "                                   (barolocal.loc['pw03','Y']-wellinfo['UTMNorthing'])**2 + \n",
      "                                   (barolocal.loc['pw03','Z']-wellinfo['G_Elev_m'])**2)\n",
      "    wellinfo['pw10'] = np.sqrt((barolocal.loc['pw10','X']-wellinfo['UTMEasting'])**2 + \n",
      "                                   (barolocal.loc['pw10','Y']-wellinfo['UTMNorthing'])**2 + \n",
      "                                   (barolocal.loc['pw10','Z']-wellinfo['G_Elev_m'])**2)\n",
      "    wellinfo['pw19'] = np.sqrt((barolocal.loc['pw19','X']-wellinfo['UTMEasting'])**2 + \n",
      "                                   (barolocal.loc['pw19','Y']-wellinfo['UTMNorthing'])**2 + \n",
      "                                   (barolocal.loc['pw19','Z']-wellinfo['G_Elev_m'])**2)\n",
      "    wellinfo['closest_baro'] = wellinfo[['pw03','pw10','pw19']].T.idxmin()\n",
      "    return wellinfo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "New Well Import"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def new_xle_imp(infile):\n",
      "    # open text file\n",
      "    with open(infile) as fd:\n",
      "        # parse xml\n",
      "        obj = xmltodict.parse(fd.read(),encoding=\"ISO-8859-1\")\n",
      "    # navigate through xml to the data\n",
      "    wellrawdata = obj['Body_xle']['Data']['Log']\n",
      "    # convert xml data to pandas dataframe\n",
      "    f = pd.DataFrame(wellrawdata)\n",
      "    # get header names and apply to the pandas dataframe\n",
      "    f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()] = f['ch1']\n",
      "    f[str(obj['Body_xle']['Ch2_data_header']['Identification']).title()] = f['ch2']\n",
      "    # add extension-free file name to dataframe\n",
      "    f['name'] = getfilename(infile)\n",
      "    # combine Date and Time fields into one field\n",
      "    f['DateTime'] = pd.to_datetime(f.apply(lambda x: x['Date'] + ' ' + x['Time'], 1))\n",
      "    f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()] = f[str(obj['Body_xle']['Ch1_data_header']['Identification']).title()].convert_objects(convert_numeric=True)\n",
      "    f[str(obj['Body_xle']['Ch2_data_header']['Identification']).title()] = f[str(obj['Body_xle']['Ch2_data_header']['Identification']).title()].convert_objects(convert_numeric=True)\n",
      "    f = f.reset_index()\n",
      "    f = f.set_index('DateTime')\n",
      "    f = f.drop(['Date','Time','@id','ch1','ch2','index','ms'],axis=1)\n",
      "    return f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Resample"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hourly_resample(df,bse=0):\n",
      "    df = df.resample('1Min', how='mean')\n",
      "    df = df.interpolate(method='time')\n",
      "    df = df.resample('60Min', how='mean', base=bse)\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fcl(df, dtObj):\n",
      "    '''\n",
      "    finds closest date index in a dataframe to a date object\n",
      "    \n",
      "    df = dataframe\n",
      "    dtObj = date object\n",
      "    '''\n",
      "    return df.iloc[np.argmin(np.abs(df.index.to_pydatetime() - dtObj))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Import Relevant Data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Well Information"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wellinfo = pd.read_csv(Drive + ':\\\\Snake Valley Water\\\\Transducer Data\\\\Database Archive\\\\well.csv',header=0,index_col=0)\n",
      "wellinfo[\"G_Elev_m\"] = wellinfo[\"GroundElevation\"]/3.2808\n",
      "wellinfo[\"Well\"] = wellinfo['Well'].apply(lambda x: str(x).lower().strip())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "folder = Drive + ':\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\2015\\\\2015 q1'\n",
      "wellinfofile = Drive + ':\\\\Snake Valley Water\\\\Transducer Data\\\\Database Archive\\\\well.csv'\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Manual Water Levels"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "manualwls = Drive + \":\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\All tape measurements.csv\"\n",
      "manual = pd.read_csv(manualwls, skiprows=0, parse_dates=0, \n",
      "                     index_col=\"DateTime\", engine=\"python\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Barometric Pressure Data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Compilation of Barometric Pressure Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pw03baro_append = Drive + \":\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\2015\\\\2015 q1\\\\pw03 baro 2015-03-04.xle\"\n",
      "pw10baro_append = Drive + \":\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\2015\\\\2015 q1\\\\pw10 baro 2015-03-04.xle\"\n",
      "pw19baro_append = Drive + \":\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\2015\\\\2015 q1\\\\pw19 baro 2015-03-05.xle\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pw03baro = Drive + \":\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\pw03baro.csv\"\n",
      "pw10baro = Drive + \":\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\pw10baro.csv\"\n",
      "pw19baro = Drive + \":\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\pw19baro.csv\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "appendomatic(pw03baro_append,pw03baro)\n",
      "appendomatic(pw10baro_append,pw10baro)\n",
      "appendomatic(pw19baro_append,pw19baro)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# duplicated to update changes made by appendomatic\n",
      "pw03baro = pd.read_csv(Drive + \":\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\pw03baro.csv\",index_col='DateTime',parse_dates=True)\n",
      "pw10baro = pd.read_csv(Drive + \":\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\pw10baro.csv\",index_col='DateTime',parse_dates=True)\n",
      "pw19baro = pd.read_csv(Drive + \":\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\pw19baro.csv\",index_col='DateTime',parse_dates=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#plot here\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pw03baro['pw03'] = pw03baro['Level']\n",
      "pw03baro = hourly_resample(pw03baro)\n",
      "pw10baro['pw10'] = pw10baro['Level']\n",
      "pw10baro = hourly_resample(pw10baro)\n",
      "pw19baro['pw19'] = pw19baro['Level']\n",
      "pw19baro = hourly_resample(pw19baro)\n",
      "baro = pd.merge(pw03baro,pw10baro,how=\"outer\",left_index=True,right_index=True)\n",
      "baro = pd.merge(baro,pw19baro,how=\"outer\",left_index=True,right_index=True)\n",
      "baro.dropna(axis=0,inplace=True)\n",
      "baro['integr'] = 0 #for vented transducers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wellinfo = barodistance(wellinfo)\n",
      "wellinfo = make_files_table(folder, wellinfofile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Water Level Tranducer Data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Import Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "infile = Drive + \":\\\\Snake Valley Water\\\\Transducer Data\\\\Raw_data_archive\\\\2015\\\\2015 q1\\\\pw01a 2015-03-03.xle\"\n",
      "f = new_xle_imp(infile)\n",
      "wellid = wellinfo[wellinfo['Well']==getfilename(infile)[0:5].strip().lower()].index.values[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Match Barometric Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "barologtype = {1:'integr',2:'pw03',3:'pw10',4:'pw19',5:'pw25',6:'twin',7:'courthouse'}\n",
      "bse = f.index.to_datetime().minute[0]\n",
      "btype = wellinfo.loc[wellid,'closest_baro']\n",
      "b = baro[btype]\n",
      "#btype = int(wellinfo[wellinfo.index==wellid]['BaroLoggerTypeID'])\n",
      "#b = baro[barologtype.get(btype)]\n",
      "b = b.to_frame()\n",
      "b = hourly_resample(b, bse)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g = pd.merge(f,b,left_index=True,right_index=True,how='inner')\n",
      "g.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "Index([u'Level', u'Temperature', u'name', u'pw19'], dtype='object')"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g['wtr_abv'] = g['Level'] - g[barologtype.get(btype)]\n",
      "g['wtr_abv'].isnull().sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Get Barometric Efficiency"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "be = wellinfo[wellinfo.index==wellid]['BaroEfficiency']\n",
      "be = be.iloc[0]\n",
      "g['BaroEfficiencyCorrected'] = g['wtr_abv'] + be*g['wtr_abv']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g['DeltaLevel'] = g['BaroEfficiencyCorrected'] - g['BaroEfficiencyCorrected'][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Remove Drift"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://stackoverflow.com/questions/9877391/how-to-get-the-closest-single-row-after-a-specific-datetime-index-using-python-p\n",
      "http://stackoverflow.com/questions/15115547/find-closest-row-of-dataframe-to-given-time-in-pandas"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print max(f.index.to_datetime())\n",
      "print min(f.index.to_datetime())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g['MeasuredDTW'] = fcl(manual[manual['WellID']== wellid],min(g.index.to_datetime()))[1]-g['DeltaLevel']\n",
      "lastdtw = g['MeasuredDTW'][-1]\n",
      "\n",
      "lastdtw"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wellinfo.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "last = fcl(manual[manual['WellID']== wellid],max(g.index.to_datetime()))[1]\n",
      "first = fcl(manual[manual['WellID']== wellid],min(g.index.to_datetime()))[1]\n",
      "lastg = float(g[g.index.to_datetime()==max(g.index.to_datetime())]['MeasuredDTW'].values)\n",
      "driftlen = len(g.index)\n",
      "g['last_diff_int'] = (last-lastg)/driftlen\n",
      "g['DriftCorrection'] = g['last_diff_int'].cumsum()-g['last_diff_int']\n",
      "g['DTWBelowCasing'] = g['DriftCorrection'] + g['MeasuredDTW']\n",
      "g['DTWBelowGroundSurface'] = g['DTWBelowCasing'] - wellinfo[wellinfo.index==wellid]['Offset']\n",
      "g['WaterElevation'] = wellinfo[wellinfo.index==wellid]['GroundElevation'] - g['DTWBelowGroundSurface']\n",
      "print last\n",
      "print first\n",
      "print last-lastg\n",
      "print last_diff_int\n",
      "print g['drift_corr']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}