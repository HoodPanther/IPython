{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the Scientific Python (scipy) stack tools to generate flow duration curves from current USGS NWIS data.\n",
    "\n",
    "Using recipes from this notebook, you can make:\n",
    "* USGS Station Summaries\n",
    "* Flow duration curves\n",
    "* Iterative import and compilation of USGS station information and data\n",
    "* boxplots using pandas\n",
    "* iterative charts (one monthly summary boxplot per station)\n",
    "* Gantt charts of USGS stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this for some great `pandas` applications:\n",
    "http://earthpy.org/time_series_analysis_with_pandas_part_2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "from pyproj import Proj, transform\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "from datetime import datetime, date, timedelta\n",
    "import mechanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "huc = '16010203'\n",
    "stationhtml = \"http://waterservices.usgs.gov/nwis/site/?format=rdb,1.0&huc=\"+str(huc)+\"&siteType=GW&hasDataTypeCd=gw\"\n",
    "response = urllib2.urlopen(stationhtml)\n",
    "html = response.read()\n",
    "skip = html[:html.rfind('#\\n')+2].count('\\n')\n",
    "skiplist = range(0,skip)\n",
    "skiplist.append(skip+1)\n",
    "sites = pd.read_table(stationhtml,sep=\"\\t\",skiprows=skiplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations = list(sites['site_no'].values)\n",
    "stations = [str(i) for i in stations]\n",
    "stations = ', '.join(stations)\n",
    "stations = stations.replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stationinfohtml = \"http://waterservices.usgs.gov/nwis/site/?format=rdb&sites=\"+stations+\"&siteOutput=expanded\"\n",
    "response = urllib2.urlopen(stationinfohtml)\n",
    "html = response.read()\n",
    "skip = html[:html.rfind('#\\n')+2].count('\\n')\n",
    "skiplist = range(0,skip)\n",
    "skiplist.append(skip+1)\n",
    "siteinfo = pd.read_table(stationinfohtml, sep=\"\\t\",skiprows=skiplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'agency_cd', u'site_no', u'station_nm', u'site_tp_cd', u'lat_va',\n",
       "       u'long_va', u'dec_lat_va', u'dec_long_va', u'coord_meth_cd',\n",
       "       u'coord_acy_cd', u'coord_datum_cd', u'dec_coord_datum_cd',\n",
       "       u'district_cd', u'state_cd', u'county_cd', u'country_cd',\n",
       "       u'land_net_ds', u'map_nm', u'map_scale_fc', u'alt_va', u'alt_meth_cd',\n",
       "       u'alt_acy_va', u'alt_datum_cd', u'huc_cd', u'basin_cd', u'topo_cd',\n",
       "       u'instruments_cd', u'construction_dt', u'inventory_dt',\n",
       "       u'drain_area_va', u'contrib_drain_area_va', u'tz_cd', u'local_time_fg',\n",
       "       u'reliability_cd', u'gw_file_cd', u'nat_aqfr_cd', u'aqfr_cd',\n",
       "       u'aqfr_type_cd', u'well_depth_va', u'hole_depth_va', u'depth_src_cd',\n",
       "       u'project_no'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siteinfo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getelev(x):\n",
    "    elev = \"http://ned.usgs.gov/epqs/pqs.php?x=\"+str(x[0])+\"&y=\"+str(x[1])+\"&units=Meters&output=xml\"\n",
    "    response = urllib2.urlopen(elev)\n",
    "    html = response.read()\n",
    "    d = xmltodict.parse(html)\n",
    "    return float(d['USGS_Elevation_Point_Query_Service']['Elevation_Query']['Elevation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getwlelev(x):\n",
    "    return x[1] - (x[0]/3.2808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qqq(x):\n",
    "    x.rstrip().lstrip()\n",
    "    j = x.split(' ')\n",
    "    a = j[0][:1]\n",
    "    b = j[0][1:]\n",
    "    c = j[1][:1]\n",
    "    d = j[1][1:]\n",
    "    e = [a,b,c,d,j[2],j[3],j[4],j[5],j[6]]\n",
    "    \n",
    "    NS = int(e[1].replace(',',''))\n",
    "    EW = int(e[3].replace(',',''))\n",
    "    qc = e[4]\n",
    "    d1 = e[0]\n",
    "    d2 = e[2]\n",
    "    dic1 = {'NE':'a','NW':'b','SW':'c','SE':'d'}\n",
    "    qcdDict = {'E4S':'d','E4N':'a','N4E':'a','N4W':'b','W4N':'b','W4S':'c','S4W':'c','S4E':'d'}\n",
    "    dic2 = {'a':'b','b':'a','c':'d','d':'c'}\n",
    "    dic3 = {'a':'d','b':'c','c':'b','d':'a'}\n",
    "    dic4 = {'a':'c','b':'d','c':'a','d':'b'}\n",
    "    if qc[-1]=='4':\n",
    "        if qc[0]=='N' or qc[0]=='S':\n",
    "            qcd = qc+d2\n",
    "        elif qc[0]=='E' or qc[0]=='W':\n",
    "            qcd = qc+d1\n",
    "        q1 = qcdDict.get(qcd,'x')\n",
    "    elif qc in ('NE','NW','SW','SE'):\n",
    "        q1 = dic1.get(qc)\n",
    "    else:\n",
    "        print \"invalid quarter\"\n",
    "        q1 = 'X'\n",
    "    if NS < 1320:\n",
    "        if EW <1320:\n",
    "            q2 = q1\n",
    "        elif EW >1320:\n",
    "            qd2 = {'a':'b','b':'a','c':'d','d':'c'}\n",
    "            q2 = dic2.get(q1,'x')\n",
    "    elif NS > 1320:\n",
    "        if EW <1320:\n",
    "            q2 = dic3.get(q1,'x')\n",
    "        elif EW >1320:\n",
    "            q2 = dic4.get(q1,'x')\n",
    "    else:\n",
    "        q2 = 'X'\n",
    "\n",
    "    if NS < 660 or (NS > 1320 and NS < 1980):\n",
    "        if (EW < 660) or (EW > 1320 and EW < 1980):\n",
    "            q3 = q1\n",
    "        elif (EW > 660 and EW < 1320) or (EW > 1980 and EW < 2640):\n",
    "            q3 = dic2.get(q1,'x')\n",
    "    elif (NS > 660 and NS < 1320) or (NS > 1980 and NS < 2640):\n",
    "        if (EW < 660) or (EW > 1320 and EW < 1980):\n",
    "            q3 = dic3.get(q1,'x')\n",
    "        elif (EW > 660 and EW < 1320) or (EW > 1980 and EW < 2640):\n",
    "            q3 = dic4.get(q1,'x')\n",
    "    else:\n",
    "        q3 = 'X'\n",
    "    Tn = e[6][:-1].rjust(2)\n",
    "    Rn = e[7][:-1].rjust(2)\n",
    "    Sec = e[5].rjust(2)\n",
    "    TRd = e[6][-1]+e[7][-1]\n",
    "    TR = dic1.get(TRd).upper()\n",
    "    CAD = '('+TR+'-'+Tn+'-'+Rn+')'+Sec+q1+q2+q3+'-1'\n",
    "    return CAD                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getPLSS(desc):\n",
    "    PLSStitlen = len(\"PLSS Description is </font><font size='4'> \")\n",
    "    PLSSbeg = desc.find(\"PLSS Description is </font><font size='4'>\")+PLSStitlen\n",
    "    PLSSend = desc[PLSSbeg:].find(\" <br> \")+PLSSbeg\n",
    "    PLSSdesc = desc[PLSSbeg:PLSSend]\n",
    "    QRTRend = desc.find(\" of the above Section </font></center><p>\")\n",
    "    QRTRbeg = desc.find(\"The point is found in the \")+len(\"The point is found in the \")\n",
    "    QRTRdesc = desc[QRTRbeg:QRTRend]\n",
    "    SecBeg = PLSSdesc.find(\"Section \")+len(\"Section \")\n",
    "    SecEnd = PLSSdesc.find(\", Township \")\n",
    "    Section = int(PLSSdesc[SecBeg:SecEnd])\n",
    "    TownBeg = SecEnd + len(\", Township \")\n",
    "    TownEnd = PLSSdesc.find(\", Range \")\n",
    "    Township = PLSSdesc[TownBeg:TownEnd]\n",
    "    RangeBeg = TownEnd + len(\", Range \")\n",
    "    RangeEnd = RangeBeg + PLSSdesc[RangeBeg:].find(\", \")\n",
    "    Range = PLSSdesc[RangeBeg:RangeEnd]\n",
    "\n",
    "    PLSS = PLSSdesc.replace(\"South \",\"S\")\n",
    "    PLSS = PLSS.replace(\"West \",\"W\")\n",
    "    PLSS = PLSS.replace(\"North \",\"N\")\n",
    "    PLSS = PLSS.replace(\"East \",\"E\")\n",
    "    PLSS = PLSS.replace(\"feet \",\"\")\n",
    "    PLSS = PLSS.replace(\" and \",\" \")\n",
    "    PLSS = PLSS.replace(\"from the \",\"\")\n",
    "    BM = PLSS[-6:-4]\n",
    "    PLSScn = PLSS.find(\" Corner\")\n",
    "    PLSS = PLSS[:PLSScn]\n",
    "    PLSS = PLSS.replace(',','')\n",
    "    PLSS = PLSS + \" \" +str(Section) + \" \" + Township + \" \" + Range + \" \" + BM\n",
    "    \n",
    "    try:\n",
    "        CAD = qqq(PLSS)\n",
    "    except(AttributeError,TypeError,UnboundLocalError):\n",
    "        print PLSS\n",
    "        CAD = np.nan\n",
    "    return PLSS, CAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A- 9- 1) 3bba-1\n"
     ]
    }
   ],
   "source": [
    "PLSS = 'N337 E1,117 W4 3 9N 1E SL'\n",
    "print qqq(PLSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def proj(x):\n",
    "    inProj = Proj(init='epsg:4326') #WGS84\n",
    "    outProj = Proj(init='epsg:2152') #NAD83(CSRS98) / UTM zone 12N\n",
    "    x2,y2 = transform(inProj,outProj,x[0],x[1])\n",
    "    return x2, y2\n",
    "\n",
    "siteinfo['UTM_X'], siteinfo['UTM_Y'] = zip(*siteinfo[['dec_long_va','dec_lat_va']].apply(lambda x: proj(x),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def USGSID(x):\n",
    "    def dms(dec):\n",
    "        DD = str(int(abs(dec)))\n",
    "        MM = str(int((abs(dec) - int(DD))*60)).zfill(2)\n",
    "        SS = str(int(round((((abs(dec) - int(DD))*60) - int(MM))*60, 0))).zfill(2)\n",
    "        return DD+MM+SS\n",
    "    return 'UT'+dms(x[1])+dms(x[0])+'01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getwellinfo(x):\n",
    "    request = mechanize.Request(\"http://maps.waterrights.utah.gov/asp/location.asp\")\n",
    "    response = mechanize.urlopen(request)\n",
    "    forms = mechanize.ParseResponse(response, backwards_compat=False)\n",
    "    response.close()\n",
    "    form = forms[0]\n",
    "    form[\"UTMx\"]= str(x[0])\n",
    "    form[\"UTMy\"]= str(x[1])\n",
    "    form[\"datumutm\"]=[\"NAD83\"]\n",
    "    desc =  mechanize.urlopen(form.click()).read()\n",
    "    try:\n",
    "        PLSS, CAD = getPLSS(desc)\n",
    "    except(ValueError):\n",
    "        PLSS, CAD = np.nan, np.nan\n",
    "    return PLSS, CAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://wwwsearch.sourceforge.net/mechanize/forms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def winmatch(x):\n",
    "    request = mechanize.Request(\"http://waterrights.utah.gov/wellinfo/wellsearch.asp\")\n",
    "    response = mechanize.urlopen(request)\n",
    "    forms = mechanize.ParseResponse(response, backwards_compat=False)\n",
    "    response.close()\n",
    "    form = forms[0] \n",
    "    #print form\n",
    "    form[\"mainoption\"]=[\"radius\"]\n",
    "    form[\"SearchRadius\"]=\"2000\"\n",
    "    form[\"option\"]=[\"UTM\"]\n",
    "    form[\"xUTM\"]=str(x[0])\n",
    "    form[\"yUTM\"]=str(x[1])\n",
    "    if np.isfinite(x[2]):\n",
    "        form[\"DateRange\"]=[\"on\"]\n",
    "        form[\"edtDrillBef\"]=str(int(str(x[2])[:4])+5)\n",
    "        form[\"edtDrillAft\"]=str(int(str(x[2])[:4])-5)\n",
    "        #print str(int(str(x[2])[:4]))\n",
    "    if np.isfinite(x[3]):\n",
    "        form[\"DepthRange\"]=[\"on\"]\n",
    "        form[\"edtDrillBel\"]=str(int(x[3])-10)\n",
    "        form[\"edtDrillAbo\"]=str(int(x[3])+30)\n",
    "        #print str(str(int(x[3])))\n",
    "    win =  mechanize.urlopen(form.click()).read()\n",
    "    winbeg = win.find('WIN=')\n",
    "    if winbeg == -1:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan \n",
    "    else:\n",
    "        wintabeg=win.find('<table',win.find('<table')+5)\n",
    "        wintaend=win.find('</table>')\n",
    "        winmatches = pd.read_html(win[wintabeg:wintaend], header=0, skiprows=0)\n",
    "        winmatches = winmatches[0]\n",
    "        winDic = {u'WRNUM/Appl. No.':'WRNUM',u'Distance From Point (ft)':'DIST',u'Diameter':'Diam',u'Depth':'TD',\n",
    "          u'Drilled Date':'DrillDate',u'Location(link to Log)':'Locatio',u'WIN':'WIN',u'Geologic Log':'Log'}\n",
    "        winmatches.rename(columns=winDic,inplace=True)\n",
    "        return winmatches.ix[0,0], winmatches.ix[0,1], winmatches.ix[0,2], winmatches.ix[0,3], winmatches.ix[0,4], winmatches.ix[0,5], int(winmatches.ix[0,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'25-10783',\n",
       " 882,\n",
       " 155.0,\n",
       " 155.0,\n",
       " u'08/25/1967',\n",
       " u'S 975 W 4572 NE 28 10N 1E SL',\n",
       " 432043,\n",
       " nan]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winDic = {u'WRNUM/Appl. No.':'WRNUM',u'Distance From Point (ft)':'DIST',u'Diameter':'Diam',u'Depth':'TD',\n",
    "          u'Drilled Date':'DrillDate',u'Location(link to Log)':'Locatio',u'WIN':'WIN',u'Geologic Log':'Log'}\n",
    "winmatches.rename(columns=winDic,inplace=True)\n",
    "list(winmatches.ix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siteinfo['Elev'] = siteinfo[['dec_long_va','dec_lat_va']].apply(lambda x: getelev(x),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siteinfo['USGSid'] = siteinfo[['dec_long_va','dec_lat_va']].apply(lambda x: USGSID(x),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N89 E1320 S4 31 12N 1E SL\n"
     ]
    }
   ],
   "source": [
    "siteinfo['PLSS'], siteinfo['CAD'] = zip(*siteinfo[['UTM_X','UTM_Y']].apply(lambda x: getwellinfo(x),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siteinfo['WRNUM'], siteinfo['Dist'], siteinfo['Diam'], siteinfo['DepthWR'], siteinfo['DrillDate'], siteinfo['Loc'], siteinfo['WIN'] = zip(*siteinfo[['UTM_X','UTM_Y','construction_dt','well_depth_va']].apply(lambda x: winmatch(x),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "siteinfo.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datahtml = \"http://waterservices.usgs.gov/nwis/gwlevels/?format=rdb&sites=\"+stations+\"&startDT=1800-01-01&endDT=\"+str(datetime.today().year)+\"-\"+str(datetime.today().month).zfill(2)+\"-\"+str(datetime.today().day).zfill(2)\n",
    "response = urllib2.urlopen(datahtml)\n",
    "html = response.read()\n",
    "skip = html[:html.rfind('#\\n')+2].count('\\n')\n",
    "skiplist = range(0,skip)\n",
    "skiplist.append(skip+1)\n",
    "data = pd.read_table(datahtml, sep=\"\\t\",skiprows=skiplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.drop([u'agency_cd', u'site_tp_cd'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stationWL = pd.merge(data, siteinfo, on='site_no', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NAD83'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationWL['dec_coord_datum_cd'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stationWL = stationWL[~stationWL['lev_status_cd'].isin(['Z', 'R', 'V', 'P', 'O', 'F', 'W', 'G', 'S', 'C', 'E', 'N'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stationWL['wlelev'] = stationWL[['lev_va','Elev']].apply(lambda x: getwlelev(x),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = stationWL.wlelev.values\n",
    "x = stationWL.UTM_X.values\n",
    "y = stationWL.UTM_Y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://connor-johnson.com/2014/03/20/simple-kriging-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stackoverflow.com/questions/31124930/ckdtree-vs-dsearchn?lq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "A = np.loadtxt('A.txt')\n",
    "B = np.loadtxt('B.txt')\n",
    "tree = cKDTree( B[:,[1,2,3]] )\n",
    "d, inds = tree.query( A[:,[1,2,3]], k=1, p=2)\n",
    "B_new = B[inds]\n",
    "xyz_near = np.hstack(( B_new[:,0:4], A[:,0:4] ))\n",
    "\n",
    "for j, a in enumerate(A):\n",
    "    # compute 2-norms from each point in B to a\n",
    "    dd = np.sqrt(((a[1:] - B[:,1:])**2).sum(axis=1))\n",
    "    # find closest point\n",
    "    jx = np.argmin(dd)\n",
    "    # check solution\n",
    "    assert inds[j] == jx\n",
    "    assert np.allclose(d[j], dd.min())\n",
    "    # check it is unique\n",
    "    assert (dd[jx+1:] > d[j]).all()\n",
    "    assert (dd[:jx] > d[j]).all()\n",
    "\n",
    "print(\"All OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script will generate a series of box and whisker plots and save them in a pdf. It makes a box plot for each station, breaking the data into monthly intervals.  Make sure to change the directory name in the script so it ends up in a recognizable place on your computer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dictionary of integers and their month equivalent\n",
    "months = {'1':'Jan.', '2':'Feb.', '3':'Mar.', '4':'Apr.', '5':'May', '6':'Jun.', \n",
    "         '7':'Jul.', '8':'Aug.', '9':'Sep.', '10':'Oct.', '11':'Nov.', '12':'Dec.', 'Total':'Total'}\n",
    "# create empty dictionary to hold pandas Dataframes\n",
    "j = {}\n",
    "\n",
    "\n",
    "with PdfPages(rootname + 'station_boxplots.pdf') as pdfs:\n",
    "    ymax = 10000\n",
    "    ymin = 0.01\n",
    "    for i in range(len(sites)):\n",
    "        # make a dataframe containing summary statistics and store it in the j dictionary\n",
    "        j[sites[i]] = USGS_Site_Data.groupby('mon')[sites[i]].agg({'min':np.min, 'mean':np.mean, \n",
    "                                                                   'median':np.median, 'max':np.max, 'std':np.std, \n",
    "                                                                   'cnt':(lambda x: np.count_nonzero(~np.isnan(x)))}).reset_index()\n",
    "        # make a list of the custom lables you will use for your boxplot; this one will show the number of samples used to make the plot\n",
    "        labs = [months[(str(j[sites[i]]['mon'][b]))] + \" (n=\" + str(int(j[sites[i]]['cnt'][b])) + \")\" for b in range(len(j[sites[i]]))]\n",
    "        # designate the location of each custom label\n",
    "        tickloc = [b+1 for b in range(len(j[sites[i]]['mon']))]\n",
    "        \n",
    "        plt.figure()\n",
    "        USGS_Site_Data.boxplot(column=sites[i],by='mon', rot=70)\n",
    "        strtdt = str(USGS_Site_Info.ix[sites[i],'start_date'])[0:10]\n",
    "        findt = str(USGS_Site_Info.ix[sites[i],'fin_date'])[0:10]\n",
    "        siteName = USGS_Site_Info.ix[sites[i],'name'].title() \n",
    "        plt.title( siteName + ' (' + sites[i] + ')  ' + strtdt + ' to ' + findt )\n",
    "        plt.suptitle('')\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('Discharge (cfs)')\n",
    "        plt.ylim((ymin,ymax))\n",
    "        plt.xlabel('Month')\n",
    "        # here is where your lists for the custom label come into play\n",
    "        plt.xticks(tickloc, labs)\n",
    "        \n",
    "        pdfs.savefig()\n",
    "\n",
    "        plt.close()\n",
    "    # Save metadata of the pdf so you can find it later\n",
    "    d = pdfs.infodict()\n",
    "    d['Title'] = 'Monthly Station USGS Boxplots UMSS'\n",
    "    d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "    d['Subject'] = 'Boxplots of several USGS Surface Stations'\n",
    "    d['Keywords'] = 'USGS Surface NWIS Boxplot'\n",
    "    d['CreationDate'] = datetime.today()\n",
    "    d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few of the boxplots so you can see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,3):\n",
    "    j[sites[i]] = USGS_Site_Data.groupby('mon')[sites[i]].agg([np.min, np.mean, np.median, np.max, np.std, np.size]).reset_index()\n",
    "    # make a list of the custom lables you will use for your boxplot; this one will show the number of samples used to make the plot\n",
    "    labs = [months[(str(j[sites[i]]['mon'][b]))] + \" (n=\" + str(int(j[sites[i]]['size'][b])) + \")\" for b in range(len(j[sites[i]]))]\n",
    "    # designate the location of each custom label\n",
    "    tickloc = [b+1 for b in range(len(j[sites[i]]['mon']))]\n",
    "\n",
    "    plt.figure()\n",
    "    USGS_Site_Data.boxplot(column=sites[i],by='mon', rot=70)\n",
    "    plt.title(USGS_Site_Info.ix[sites[i],'name'].title() + ' (' + sites[i] + ')')\n",
    "    plt.suptitle('')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('Discharge (cfs)')\n",
    "    plt.ylim((ymin,ymax))\n",
    "    plt.xlabel('Month')\n",
    "    # here is where your lists for the custom label come into play\n",
    "    plt.xticks(tickloc, labs)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will generate boxplots showing all of the station data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This script summarizes discharge for all sites and limits the number of box plots on one graph to the n variable\n",
    "j=0\n",
    "with PdfPages(rootname + 'sum_boxplots.pdf') as pdf:\n",
    "    while j < len(sites):\n",
    "        ymax = 10000\n",
    "        ymin = 0.01\n",
    "        n=10\n",
    "        # if statement allows for uneven number of sites on last page\n",
    "        if j+n >= len(sites):\n",
    "            plt.figure()\n",
    "            USGS_Site_Data[sites[j:-1]].plot(kind='box')\n",
    "            plt.title('Sites '+sites[j]+' to '+sites[-1] )\n",
    "            plt.yscale('log')\n",
    "            plt.xlabel('USGS Site')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel('discharge (cfs)')\n",
    "            plt.ylim((ymin,ymax))\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            j = j+n\n",
    "        else:\n",
    "            plt.figure()\n",
    "            USGS_Site_Data[sites[j:j+n]].plot(kind='box')\n",
    "            plt.title('Sites '+sites[j]+' to '+sites[j+n] )\n",
    "            plt.yscale('log')\n",
    "            plt.xlabel('USGS Site')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel('discharge (cfs)')\n",
    "            plt.ylim((ymin,ymax))\n",
    "            pdf.savefig()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            j = j+n\n",
    "        # Save metadata of the pdf so you can find it later\n",
    "        d = pdf.infodict()\n",
    "        d['Title'] = 'Summary USGS Boxplots UMSS'\n",
    "        d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "        d['Subject'] = 'Boxplots of several USGS Surface Stations'\n",
    "        d['Keywords'] = 'USGS Surface NWIS Boxplot'\n",
    "        d['CreationDate'] = datetime.today()\n",
    "        d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also produce hydrographs of each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xmax = USGS_Site_Data.index.astype(datetime).values[-1]\n",
    "xmin = USGS_Site_Data.index.astype(datetime).values[0]\n",
    "\n",
    "pdfs = PdfPages(rootname + 'station_hydrographs.pdf')\n",
    "ymax = 10000\n",
    "ymin = 0.1\n",
    "for i in range(len(sites)):\n",
    "    x = USGS_Site_Data.index.values\n",
    "    y = USGS_Site_Data[sites[i]].values\n",
    "    plt.figure()\n",
    "    plt.plot(x,y)\n",
    "    strtdt = str(USGS_Site_Info.ix[sites[i],'start_date'])[0:10]\n",
    "    findt = str(USGS_Site_Info.ix[sites[i],'fin_date'])[0:10]\n",
    "    siteName = USGS_Site_Info.ix[sites[i],'name'].title() \n",
    "    plt.title( siteName + ' (' + sites[i] + ')  ' + strtdt + ' to ' + findt )\n",
    "    plt.suptitle('')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('Discharge (cfs)')\n",
    "    plt.ylim((ymin,ymax))\n",
    "    plt.xlabel('Year')\n",
    "    plt.xticks(np.arange(datetime(1905,1,1),xmax+timedelta(days=365.25),timedelta(days=365.25*5)),rotation=45)\n",
    "    plt.xlim(xmin,xmax)\n",
    "    pdfs.savefig()\n",
    "    plt.close()\n",
    "    # Save metadata of the pdf so you can find it later\n",
    "\n",
    "d = pdfs.infodict()\n",
    "d['Title'] = 'Monthly Station USGS Hydrographs UMSS'\n",
    "d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "d['Subject'] = 'Hydrograph of several USGS Surface Stations'\n",
    "d['Keywords'] = 'USGS Surface NWIS Hydrograph'\n",
    "d['CreationDate'] = datetime.today()\n",
    "d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.date_range(start=xmin, end=xmax, freq='5AS').year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmax = USGS_Site_Data.index[-1]\n",
    "xmin = USGS_Site_Data.index[0]\n",
    "\n",
    "plt.figure()\n",
    "ticks = pd.date_range(start=xmin, end=xmax, freq='4AS')\n",
    "USGS_Site_Data[sites[0:3]].plot(subplots=True,sharex=True,figsize=(10,8),logy=True, rot=90)\n",
    "plt.xlim(xmin,xmax)\n",
    "labs = pd.date_range(start=xmin, end=xmax, freq='4AS').year\n",
    "plt.xticks(ticks,labs)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lumped_hydro(i1,i2):\n",
    "    pdfs = PdfPages(rootname + 'station_hydrographs_lumped.pdf')\n",
    "    plt.figure()\n",
    "    ticks = pd.date_range(start=xmin, end=xmax, freq='4AS')\n",
    "    USGS_Site_Data[sites[i1:i2]].plot(subplots=True, sharex=True, figsize=(10,24),logy=True, rot=90)\n",
    "    plt.xlim(xmin,xmax)\n",
    "    labs = pd.date_range(start=xmin, end=xmax, freq='4AS').year\n",
    "    plt.xticks(ticks,labs)\n",
    "    pdfs.savefig()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lumped_hydro(0,10)\n",
    "lumped_hydro(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lumped_hydro(20,30)\n",
    "lumped_hydro(30,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will iteratively produce Flow Duration Curves for each of the stations. A flow duration curve is a <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#PPF>percent point function (ppf)</a>, displaying discharge as a function of probability of that discharge occuring. The ppf is the inverse of the better known <a href=http://www.itl.nist.gov/div898/handbook/eda/section3/eda362.htm#CDF>cumulative distribution function (cdf)</a>. See <a href=http://pubs.usgs.gov/wsp/1542a/report.pdf>this USGS publication</a> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with PdfPages(rootname+'station_fdc.pdf') as pdf:\n",
    "    ymax = 10000\n",
    "    ymin = 0.01\n",
    "    for i in range(len(sites)):\n",
    "        plt.figure()\n",
    "        fdc_simple(USGS_Site_Data,sites[i],1900,2015)\n",
    "        fdc_simple(USGS_Site_Data,sites[i],1900,1970)\n",
    "        fdc_simple(USGS_Site_Data,sites[i],1970,2015)\n",
    "        plt.ylim(0.01,10000)\n",
    "        plt.xlim(-.05,1.05)\n",
    "        plt.grid(which = 'both')\n",
    "        plt.legend()\n",
    "        plt.xlabel('probability that discharge was exceeded or equaled')\n",
    "        plt.title('Flow duration curve for ' + str(USGS_Site_Info['name'][i]).title() + ' ('+ sites[i] +')'+'\\n'+\n",
    "                  'Record: ' + str(USGS_Site_Info['start_date'][i])[0:10] + ' to ' + str(USGS_Site_Info['fin_date'][i])[0:10])\n",
    "        plt.yscale('log')\n",
    "        plt.ylabel('discharge (cfs)')\n",
    "        plt.xticks(np.arange(0,1.05,0.05))\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "    # Save metadata of the pdf so you can find it later\n",
    "    d = pdf.infodict()\n",
    "    d['Title'] = 'Flow Duration Curves USGS'\n",
    "    d['Author'] = u'Paul C. Inkenbrandt\\xe4nen'\n",
    "    d['Subject'] = 'Flow Duration Curves of several USGS Surface Stations'\n",
    "    d['Keywords'] = 'USGS Surface NWIS FDC Flow Duration'\n",
    "    d['CreationDate'] = datetime.today()\n",
    "    d['ModDate'] = datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For brevity, here is an example plot from the output of the script above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fdc_simple(USGS_Site_Data,sites[38],1900,2015)\n",
    "fdc_simple(USGS_Site_Data,sites[38],1900,1970)\n",
    "fdc_simple(USGS_Site_Data,sites[38],1970,2015)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid(which = 'both')\n",
    "plt.xlabel('% of time that indicated discharge was exceeded or equaled')\n",
    "plt.ylabel('discharge (cfs)')\n",
    "plt.xticks(np.arange(0.0,1.05,0.05))\n",
    "plt.title('Flow duration curve for ' + str(USGS_Site_Info['name'][38]) + ' ('+ sites[i] +')'+'\\n'+ \n",
    "          'Record: ' + str(USGS_Site_Info['start_date'][i])[0:10] + ' to ' + str(USGS_Site_Info['fin_date'][i])[0:10])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some sad attempts to model the flow duration curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site = sites[28]\n",
    "df = USGS_Site_Data[[site]]\n",
    "begyear = 1900\n",
    "endyear = 2015\n",
    "data = df[(df.index.to_datetime() > pd.datetime(begyear,1,1))&(df.index.to_datetime() < pd.datetime(endyear,1,1))]\n",
    "data = data.dropna()\n",
    "\n",
    "data['doy']=data.index.dayofyear\n",
    "dailyavg = data[site].groupby(data['doy']).mean()\n",
    "\n",
    "\n",
    "data = np.sort(dailyavg)\n",
    "\n",
    "mean = np.mean(data)\n",
    "std = np.std(data)\n",
    "f = [(data[i]) for i in range(len(data))]\n",
    "#f = [(data[i])/mean for i in range(len(data))]\n",
    "#f = [(data[i]-std)/mean for i in range(len(data))]\n",
    "\n",
    "ranks = sp.rankdata(f, method='average')\n",
    "ranks = ranks[::-1]\n",
    "prob = [(ranks[i]/(len(f)+1)) for i in range(len(f)) ]\n",
    "\n",
    "x = prob\n",
    "y = f\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y,x,label=site,color='blue')\n",
    "#plt.yscale('log')\n",
    "#plt.xlim(-.05,1.05)\n",
    "plt.grid(which = 'both')\n",
    "plt.ylabel('probability that discharge was exceeded or equaled')\n",
    "plt.xlabel('normalized discharge')\n",
    "#plt.xticks(np.arange(0,1.00,0.05))\n",
    "plt.title('Flow duration curve for ' + site + ' averaged from ' + str(begyear) + ' to ' + str(endyear))\n",
    "\n",
    "def func(x,a,b,c,d):\n",
    "    return a*np.log(x*b+c)+d\n",
    "\n",
    "par, cov = op.curve_fit(func,y,x,p0=[-0.16528617, 1.54535185, -24.70440088, 0.9])\n",
    "plt.plot(y, [par[0]*np.log(y[i]*par[1]+par[2])+par[3] for i in range(len(y))], color='red')\n",
    "print 'curve fit', sp.linregress(x,[par[0]*np.log(y[i]*par[1]+par[2])+par[3] for i in range(len(y))])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x,y,label=site,color='blue')\n",
    "#plt.yscale('log')\n",
    "#plt.xlim(-.05,1.05)\n",
    "plt.grid(which = 'both')\n",
    "plt.xlabel('probability that discharge was exceeded or equaled')\n",
    "plt.ylabel('normalized discharge')\n",
    "#plt.xticks(np.arange(0,1.00,0.05))\n",
    "plt.title('Flow duration curve for ' + site + ' averaged from ' + str(begyear) + ' to ' + str(endyear))\n",
    "\n",
    "def func2(x,a,b,c,d):\n",
    "    return a*np.exp(x*b+c)+d\n",
    "\n",
    "\n",
    "parm, covm = op.curve_fit(func2,x,y,p0=[-0.16528617, 0.02, 0.70440088, 0.9])\n",
    "plt.plot(x, [parm[0]*np.exp(x[i]*parm[1]+parm[2])+parm[3] for i in range(len(x))], color='red')\n",
    "print 'curve fit', sp.linregress(y,[parm[0]*np.exp(x[i]*parm[1]+parm[2])+parm[3] for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dic2df(dic,head):\n",
    "    df = pd.DataFrame(data=dic)\n",
    "    df = df.transpose()\n",
    "    df.columns = [str(head)+'_var1',str(head)+'_var2',str(head)+'_var3',str(head)+'_var4',str(head)+'_r2',str(head)+'_err']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = {}; m = {}; n = {}; k = {}; j = {}; p = {}\n",
    "\n",
    "for site in sites:\n",
    "    q[site] = fdcmatch(USGS_Site_Data,site,1900,2015,1,1)\n",
    "    m[site] = fdcmatch(USGS_Site_Data,site,1900,1970,1,1)\n",
    "    n[site] = fdcmatch(USGS_Site_Data,site,1970,2015,1,1)\n",
    "    k[site] = fdcmatch(USGS_Site_Data,site,1900,2015,1,0)\n",
    "    j[site] = fdcmatch(USGS_Site_Data,site,1900,1970,1,0)\n",
    "    p[site] = fdcmatch(USGS_Site_Data,site,1970,2015,1,0)\n",
    "    \n",
    "dics = [q,m,n,k,j,p]\n",
    "heads = ['all','to70','fm70','allin','to70in','fm70in']\n",
    "\n",
    "USGS_q = dic2df(q,'all')\n",
    "USGS_m = dic2df(m,'to70')\n",
    "USGS_parms = pd.merge(USGS_q,USGS_m, left_index=True, right_index=True, how='outer' )\n",
    "\n",
    "for i in range(2,6,1):\n",
    "    x = dic2df(dics[i],heads[i])\n",
    "    USGS_parms = pd.merge(USGS_parms,x, left_index=True, right_index=True, how='outer' )\n",
    "\n",
    "USGS_parms.to_clipboard()\n",
    "#USGS_finish_date = USGS_finish_date.drop([0],axis=1)\n",
    "#USGS_start_fin = pd.merge(USGS_finish_date,USGS_start_date, left_index=True, right_index=True, how='outer' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations from the modeled fdc plots can be used to estimate the discharge for a site based on data from a similar site.  However, the results are mediocre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n1 = 12\n",
    "n2 = 11\n",
    "\n",
    "USGS_Site_Data[sites[n1]+'p'] = [USGS_parms['all_var1'][n1]*np.log(USGS_Site_Data[sites[n1]][i]*USGS_parms['all_var2'][n1]+\\\n",
    "                                                                   USGS_parms['all_var3'][n1])+USGS_parms['all_var4'][n1] for i in \\\n",
    "range(len(USGS_Site_Data[sites[n1]]))]\n",
    "\n",
    "USGS_Data = USGS_Site_Data[USGS_Site_Data[sites[n1]+'p']>0]\n",
    "\n",
    "USGS_Data[sites[n2]+'d'] = [USGS_parms['allin_var1'][n2]*np.exp(USGS_Data[sites[n1]+'p'][i]*USGS_parms['allin_var2'][n2]+\\\n",
    "                                                                   USGS_parms['allin_var3'][n2])+USGS_parms['allin_var4'][n2] for i in \\\n",
    "range(len(USGS_Data[sites[n1]+'p']))]\n",
    "\n",
    "y1 = USGS_Data[sites[n2]+'d'].values\n",
    "y2 = USGS_Site_Data[sites[n2]].values\n",
    "\n",
    "x1 = USGS_Data.index.to_datetime()\n",
    "x2 = USGS_Site_Data.index.to_datetime()\n",
    "y3 = USGS_Data[sites[n1]]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x1,y1, label=\"modeled discharge\")\n",
    "plt.plot(x2,y2, label=\"actual discharge\")\n",
    "plt.plot(x1,y3, label=\"reference discharge\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "#plt.xlim(USGS_Site_Info['start_date'][n2],USGS_Site_Info['fin_date'][n2])\n",
    "plt.xlim('1/1/1970','1/1/1974')\n",
    "#plt.ylim(1,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://stackoverflow.com/questions/15408371/cumulative-distribution-plots-python <br>\n",
    "http://hydroclimpy.sourceforge.net/installation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following script if you want to see a map of your stations.  This assumes that you have the <a href=http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/>Basemap package</a> installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "X = USGS_Site_Info['longitude'].astype(float).values.tolist()\n",
    "Y = USGS_Site_Info['latitude'].astype(float).values.tolist()\n",
    "\n",
    "n = 0.05 \n",
    "m = Basemap(llcrnrlon=min(X)+n,llcrnrlat=min(Y)+n,urcrnrlon=max(X)+n,urcrnrlat=max(Y)+n,\n",
    "            resolution='h',projection='cyl',lon_0=np.mean(X),lat_0=np.mean(Y))\n",
    "m.drawrivers(color='blue',linewidth=0.5)\n",
    "m.drawcounties(color='red',linewidth=0.5)\n",
    "m.arcgisimage()\n",
    "#m.etopo(scale=0.5)\n",
    "lons = X\n",
    "lats = Y\n",
    "x,y = m(lons,lats)\n",
    "m.plot(x,y,'ro', markersize=8)\n",
    "\n",
    "#m.drawmapscale(lon=-114, lat=43.5, length=100, lon0=-114, lat0=39, barstyle='simple', units='km')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col = USGS_Site_Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects as robj\n",
    "import rpy2.rlike.container as rlc\n",
    "\n",
    "def pandas_data_frame_to_rpy2_data_frame(pDataframe):\n",
    "    orderedDict = rlc.OrdDict()\n",
    "\n",
    "    for columnName in pDataframe:\n",
    "        columnValues = pDataframe[columnName].values\n",
    "        filteredValues = [value if pd.notnull(value) else robj.NA_Real \n",
    "                          for value in columnValues]\n",
    "\n",
    "        try:\n",
    "            orderedDict[columnName] = robj.FloatVector(filteredValues)\n",
    "        except ValueError:\n",
    "            orderedDict[columnName] = robj.StrVector(filteredValues)\n",
    "\n",
    "    rDataFrame = robj.DataFrame(orderedDict)\n",
    "    rDataFrame.rownames = robj.StrVector(pDataframe.index)\n",
    "\n",
    "    return rDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USD = pandas_data_frame_to_rpy2_data_frame(USGS_Site_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
